{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbSdv5IsRNhP"
   },
   "source": [
    "# Week 8 Assignment - Q-Learning Reinforcement Learning\n",
    "## Mountain Car Problem - Complete Solution\n",
    "\n",
    "**Student:** [Your Name Here]\n",
    "**Date:** October 17, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "This notebook implements Q-Learning for the Mountain Car environment from OpenAI Gymnasium. The car must learn to reach the flag on top of the hill by building momentum through swinging back and forth.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Q-Learning**: Model-free reinforcement learning algorithm\n",
    "- **Q-Table**: Lookup table storing Q-values for state-action pairs\n",
    "- **Exploration vs Exploitation**: Balance between trying new actions and using known good actions\n",
    "- **Reward Function**: Agent receives -1 for each time step until goal is reached\n",
    "\n",
    "**Environment Details:**\n",
    "- **State Space**: Position and velocity (continuous)\n",
    "- **Action Space**: 3 discrete actions (push left, do nothing, push right)\n",
    "- **Goal**: Reach the flag at position ≥ 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX9ev-zGRNhR"
   },
   "source": [
    "## Part 1: Environment Setup and Exploration\n",
    "\n",
    "### 1.1 Import Required Libraries\n",
    "We need gymnasium for the environment, numpy for numerical operations, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9hJcZP6RNhR",
    "outputId": "48ddcde5-86a5-4722-97cc-c83e116e57a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Gymnasium version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpPIo-j2RNhR"
   },
   "source": [
    "### 1.2 Create and Explore the Environment\n",
    "\n",
    "The Mountain Car environment presents a challenging problem where the car doesn't have enough power to drive straight up the hill. Instead, it must learn to build momentum by swinging back and forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FulONprHRNhR",
    "outputId": "6bc67c6e-38fa-4344-fb76-9e683b5148a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mountain Car Environment Information:\n",
      "============================================================\n",
      "Observation Space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "  - High: [0.6  0.07]\n",
      "  - Low: [-1.2  -0.07]\n",
      "\n",
      "Action Space: Discrete(3)\n",
      "  - Number of actions: 3\n",
      "  - Actions: 0=Push Left, 1=No Push, 2=Push Right\n",
      "\n",
      "Initial State: [-0.41608578  0.        ]\n",
      "  - Position: -0.4161\n",
      "  - Velocity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=None)\n",
    "\n",
    "print(\"Mountain Car Environment Information:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"  - High: {env.observation_space.high}\")\n",
    "print(f\"  - Low: {env.observation_space.low}\")\n",
    "print(f\"\\nAction Space: {env.action_space}\")\n",
    "print(f\"  - Number of actions: {env.action_space.n}\")\n",
    "print(f\"  - Actions: 0=Push Left, 1=No Push, 2=Push Right\")\n",
    "\n",
    "state, info = env.reset()\n",
    "print(f\"\\nInitial State: {state}\")\n",
    "print(f\"  - Position: {state[0]:.4f}\")\n",
    "print(f\"  - Velocity: {state[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdpohg-3RNhR"
   },
   "source": [
    "### 1.3 Understanding the Problem\n",
    "\n",
    "**Why Q-Learning?**\n",
    "- The car cannot simply drive straight up the hill\n",
    "- It needs to learn a sequence of actions to build momentum\n",
    "- Q-Learning helps discover this optimal strategy through trial and error\n",
    "\n",
    "**Continuous to Discrete Conversion:**\n",
    "Since Q-Learning uses a table, we need to discretize the continuous state space into manageable buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rQgU-UoRNhS",
    "outputId": "b2b1708d-4772-4006-dbba-741c04aefe4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " State Space Discretization:\n",
      "============================================================\n",
      "Discrete OS Size: [40, 40]\n",
      "Bucket Size for Position: 0.0450\n",
      "Bucket Size for Velocity: 0.0035\n",
      "\n",
      "Total Q-Table Size: 40 × 40 × 3\n",
      "Total Q-values to learn: 4800\n"
     ]
    }
   ],
   "source": [
    "DISCRETE_OS_SIZE = [40, 40] \n",
    "\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE\n",
    "\n",
    "print(\" State Space Discretization:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Discrete OS Size: {DISCRETE_OS_SIZE}\")\n",
    "print(f\"Bucket Size for Position: {discrete_os_win_size[0]:.4f}\")\n",
    "print(f\"Bucket Size for Velocity: {discrete_os_win_size[1]:.4f}\")\n",
    "print(f\"\\nTotal Q-Table Size: {DISCRETE_OS_SIZE[0]} × {DISCRETE_OS_SIZE[1]} × {env.action_space.n}\")\n",
    "print(f\"Total Q-values to learn: {DISCRETE_OS_SIZE[0] * DISCRETE_OS_SIZE[1] * env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGG0dB-1RNhS"
   },
   "source": [
    "## Part 2: Q-Table Initialization\n",
    "\n",
    "### 2.1 Create the Q-Table\n",
    "\n",
    "The Q-table stores the expected future rewards for taking each action in each state. Initially, we populate it with random values between -2 and 0, since all rewards in this environment are negative until the goal is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0EgTUADRNhS",
    "outputId": "f7ee83c4-8de6-4060-b7a1-c86abb73ed91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Q-Table Initialized:\n",
      "============================================================\n",
      "Shape: (40, 40, 3)\n",
      "Size: 4800 Q-values\n",
      "\n",
      "Sample Q-values at state (10, 10):\n",
      "  Action 0 (Push Left): -1.8188\n",
      "  Action 1 (No Push): -1.9926\n",
      "  Action 2 (Push Right): -0.6240\n"
     ]
    }
   ],
   "source": [
    "q_table = np.random.uniform(\n",
    "    low=-2,\n",
    "    high=0,\n",
    "    size=(DISCRETE_OS_SIZE + [env.action_space.n])\n",
    ")\n",
    "\n",
    "print(\" Q-Table Initialized:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {q_table.shape}\")\n",
    "print(f\"Size: {q_table.size} Q-values\")\n",
    "print(f\"\\nSample Q-values at state (10, 10):\")\n",
    "print(f\"  Action 0 (Push Left): {q_table[10][10][0]:.4f}\")\n",
    "print(f\"  Action 1 (No Push): {q_table[10][10][1]:.4f}\")\n",
    "print(f\"  Action 2 (Push Right): {q_table[10][10][2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZawQZXfRNhS"
   },
   "source": [
    "### 2.2 Helper Function: Convert Continuous State to Discrete\n",
    "\n",
    "This function converts the continuous state values (position and velocity) into discrete bucket indices that we can use to look up Q-values in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vM_JxhIFRNhS",
    "outputId": "0843d274-1757-4fbd-d1cb-9f17140f2342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State Conversion Example:\n",
      "Continuous State: [-0.5  0. ]\n",
      "Discrete State: (np.int64(15), np.int64(20))\n",
      "Q-values at this state: [-0.29170937 -1.30504642 -1.77702759]\n"
     ]
    }
   ],
   "source": [
    "def get_discrete_state(state):\n",
    "    \"\"\"\n",
    "    Convert continuous state to discrete state (bucket indices).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    state : array\n",
    "        Continuous state [position, velocity]\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : Discrete state indices\n",
    "    \"\"\"\n",
    "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(int))\n",
    "\n",
    "test_state = np.array([-0.5, 0.0])\n",
    "discrete_state = get_discrete_state(test_state)\n",
    "print(f\"\\n State Conversion Example:\")\n",
    "print(f\"Continuous State: {test_state}\")\n",
    "print(f\"Discrete State: {discrete_state}\")\n",
    "print(f\"Q-values at this state: {q_table[discrete_state]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-jUEQEKRNhS"
   },
   "source": [
    "## Part 3: Q-Learning Algorithm Implementation\n",
    "\n",
    "### 3.1 Q-Learning Hyperparameters\n",
    "\n",
    "**Learning Rate (α):** Controls how much new information overrides old information\n",
    "- Higher values = faster learning but less stable\n",
    "- Lower values = slower but more stable learning\n",
    "\n",
    "**Discount Factor (γ):** Determines importance of future rewards\n",
    "- Close to 1 = long-term rewards important\n",
    "- Close to 0 = immediate rewards important\n",
    "\n",
    "**Epsilon (ε):** Controls exploration vs exploitation\n",
    "- High epsilon = more exploration (random actions)\n",
    "- Low epsilon = more exploitation (best known actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uf3GFp-URNhS",
    "outputId": "29bdb73a-d4b7-481b-b841-52529a4e1c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Q-Learning Hyperparameters:\n",
      "============================================================\n",
      "Learning Rate (α): 0.1\n",
      "Discount Factor (γ): 0.95\n",
      "Episodes: 100,000\n",
      "\n",
      "Exploration Settings:\n",
      "Initial Epsilon: 1.0\n",
      "Epsilon Decay Value: 0.000020\n",
      "Decay Period: Episodes 0.999 to 50000\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.1          \n",
    "DISCOUNT = 0.95              \n",
    "EPISODES = 100000\n",
    "\n",
    "EPSILON = 1.0                \n",
    "START_EPSILON_DECAY = 0.999      \n",
    "END_EPSILON_DECAY = EPISODES // 2  \n",
    "epsilon_decay_value = EPSILON / (END_EPSILON_DECAY - START_EPSILON_DECAY)\n",
    "\n",
    "SHOW_EVERY = 2000           \n",
    "\n",
    "print(\" Q-Learning Hyperparameters:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Learning Rate (α): {LEARNING_RATE}\")\n",
    "print(f\"Discount Factor (γ): {DISCOUNT}\")\n",
    "print(f\"Episodes: {EPISODES:,}\")\n",
    "print(f\"\\nExploration Settings:\")\n",
    "print(f\"Initial Epsilon: {EPSILON}\")\n",
    "print(f\"Epsilon Decay Value: {epsilon_decay_value:.6f}\")\n",
    "print(f\"Decay Period: Episodes {START_EPSILON_DECAY} to {END_EPSILON_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KrUs-umRNhS"
   },
   "source": [
    "### 3.2 The Q-Learning Update Formula\n",
    "\n",
    "The core of Q-Learning is the update rule:\n",
    "\n",
    "**Q_new(s,a) = Q_old(s,a) + α × [R + γ × max(Q(s',a')) - Q_old(s,a)]**\n",
    "\n",
    "Where:\n",
    "- **Q_old(s,a)**: Current Q-value for state s and action a\n",
    "- **α**: Learning rate\n",
    "- **R**: Immediate reward\n",
    "- **γ**: Discount factor\n",
    "- **max(Q(s',a'))**: Maximum Q-value for next state s'\n",
    "- **Q_new(s,a)**: Updated Q-value\n",
    "\n",
    "This formula updates our Q-values based on:\n",
    "1. The immediate reward received\n",
    "2. The estimated value of the best action in the next state\n",
    "3. The difference between our prediction and reality (temporal difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zrGbuobRNhS"
   },
   "source": [
    "### 3.3 Training Loop\n",
    "\n",
    "Now we implement the complete Q-Learning training loop. This will:\n",
    "1. Run 25,000 episodes\n",
    "2. In each episode, take actions until goal is reached or max steps exceeded\n",
    "3. Update Q-values using the Q-Learning formula\n",
    "4. Gradually reduce exploration (epsilon decay)\n",
    "5. Track rewards for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aecvgoFXRNhS",
    "outputId": "e4edb9de-4f11-465b-e5cc-a2c5be6356d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Q-Learning Training...\n",
      "============================================================\n",
      "Training for 100,000 episodes\n",
      "This may take a few minutes...\n",
      "\n",
      "Episode:      0 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 1.0000\n",
      "Episode:   2000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.9600\n",
      "Episode:   4000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.9200\n",
      "Episode:   6000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.8800\n",
      "Episode:   8000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.8400\n",
      "Episode:  10000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.8000\n",
      "Episode:  12000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.7600\n",
      "Episode:  14000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.7200\n",
      "Episode:  16000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.6800\n",
      "Episode:  18000 | Avg: -200.00 | Min: -200.00 | Max: -200.00 | Epsilon: 0.6400\n",
      "Episode:  20000 | Avg: -199.99 | Min: -200.00 | Max: -180.00 | Epsilon: 0.6000\n",
      "Episode:  22000 | Avg: -199.86 | Min: -200.00 | Max: -161.00 | Epsilon: 0.5600\n",
      "Episode:  24000 | Avg: -199.26 | Min: -200.00 | Max: -149.00 | Epsilon: 0.5200\n",
      "Episode:  26000 | Avg: -197.85 | Min: -200.00 | Max: -154.00 | Epsilon: 0.4800\n",
      "Episode:  28000 | Avg: -195.67 | Min: -200.00 | Max: -153.00 | Epsilon: 0.4400\n",
      "Episode:  30000 | Avg: -190.79 | Min: -200.00 | Max: -141.00 | Epsilon: 0.4000\n",
      "Episode:  32000 | Avg: -186.94 | Min: -200.00 | Max: -132.00 | Epsilon: 0.3600\n",
      "Episode:  34000 | Avg: -181.80 | Min: -200.00 | Max: -114.00 | Epsilon: 0.3200\n",
      "Episode:  36000 | Avg: -177.16 | Min: -200.00 | Max: -117.00 | Epsilon: 0.2800\n"
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'min': [], 'max': []}\n",
    "\n",
    "print(\"Starting Q-Learning Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPISODES:,} episodes\")\n",
    "print(f\"This may take a few minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "\n",
    "    state, info = env.reset()\n",
    "    discrete_state = get_discrete_state(state)\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > EPSILON:\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        elif new_state[0] >= env.observation_space.high[0]:\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    if END_EPSILON_DECAY >= episode >= START_EPSILON_DECAY:\n",
    "        EPSILON -= epsilon_decay_value\n",
    "\n",
    "    ep_rewards.append(episode_reward)\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        avg_reward = sum(ep_rewards[-SHOW_EVERY:]) / len(ep_rewards[-SHOW_EVERY:])\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(avg_reward)\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
    "\n",
    "        print(f\"Episode: {episode:6d} | \"\n",
    "              f\"Avg: {avg_reward:7.2f} | \"\n",
    "              f\"Min: {min(ep_rewards[-SHOW_EVERY:]):7.2f} | \"\n",
    "              f\"Max: {max(ep_rewards[-SHOW_EVERY:]):7.2f} | \"\n",
    "              f\"Epsilon: {EPSILON:.4f}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n Training completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Final epsilon: {EPSILON:.4f}\")\n",
    "print(f\"Final average reward: {aggr_ep_rewards['avg'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAwLxp_7RNhT"
   },
   "source": [
    "## Part 4: Results Analysis and Visualization\n",
    "\n",
    "### 4.1 Reward Plot Over Time\n",
    "\n",
    "This plot shows how the agent's performance improves over training. We can see:\n",
    "- **Average reward** trending upward as the agent learns\n",
    "- **Min/max bands** showing the range of performance\n",
    "- Eventually stabilizing as the agent masters the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "wyxLsyywRNhT",
    "outputId": "b4fef315-5c7d-49da-acaf-a0a0c166948e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label='Average Reward', linewidth=2)\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label='Min Reward', alpha=0.5, linestyle='--')\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label='Max Reward', alpha=0.5, linestyle='--')\n",
    "\n",
    "plt.fill_between(aggr_ep_rewards['ep'],\n",
    "                 aggr_ep_rewards['min'],\n",
    "                 aggr_ep_rewards['max'],\n",
    "                 alpha=0.2)\n",
    "\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Reward', fontsize=12)\n",
    "plt.title('Q-Learning Performance: Mountain Car Environment', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Training Progress Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Initial average reward: {aggr_ep_rewards['avg'][0]:.2f}\")\n",
    "print(f\"Final average reward: {aggr_ep_rewards['avg'][-1]:.2f}\")\n",
    "print(f\"Improvement: {aggr_ep_rewards['avg'][-1] - aggr_ep_rewards['avg'][0]:.2f}\")\n",
    "print(f\"\\nBest performance: {max(aggr_ep_rewards['max']):.2f}\")\n",
    "print(f\"Worst performance: {min(aggr_ep_rewards['min']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-4M6NsdRNhT"
   },
   "source": [
    "### 4.2 Detailed Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRVAMe9hRNhT",
    "outputId": "b9af96d1-688e-4197-e2e0-fe9936ad9b8f"
   },
   "outputs": [],
   "source": [
    "recent_rewards = ep_rewards[-1000:] \n",
    "\n",
    "print(\" Final Performance Metrics (Last 1000 Episodes):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Reward: {np.mean(recent_rewards):.2f}\")\n",
    "print(f\"Median Reward: {np.median(recent_rewards):.2f}\")\n",
    "print(f\"Std Deviation: {np.std(recent_rewards):.2f}\")\n",
    "print(f\"Min Reward: {np.min(recent_rewards):.2f}\")\n",
    "print(f\"Max Reward: {np.max(recent_rewards):.2f}\")\n",
    "\n",
    "success_threshold = -200\n",
    "successes = sum(1 for r in recent_rewards if r > success_threshold)\n",
    "success_rate = (successes / len(recent_rewards)) * 100\n",
    "\n",
    "print(f\"\\n Success Rate (reward > {success_threshold}): {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyStLmrFRNhT"
   },
   "source": [
    "## Part 5: Testing the Trained Agent\n",
    "\n",
    "### 5.1 Demonstrate Learned Policy\n",
    "\n",
    "Now let's test our trained agent by running it in the environment and observing its behavior. The agent should consistently reach the goal using the learned Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iukPoeFtRNhT",
    "outputId": "c7c298ad-4aed-4c0c-cac6-50fe0ad291c6"
   },
   "outputs": [],
   "source": [
    "print(\"Testing Trained Agent...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_episodes = 10\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    state, info = env.reset()\n",
    "    discrete_state = get_discrete_state(state)\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "    test_rewards.append(episode_reward)\n",
    "    result = \"SUCCESS\" if new_state[0] >= 0.5 else \"FAILED\"\n",
    "    print(f\"Test Episode {episode+1}: Reward = {episode_reward:6.0f} | Steps = {steps:3d} | {result}\")\n",
    "\n",
    "print(f\"\\n Test Results:\")\n",
    "print(f\"Average Reward: {np.mean(test_rewards):.2f}\")\n",
    "print(f\"Success Rate: {sum(1 for r in test_rewards if r > -200) / len(test_rewards) * 100:.0f}%\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "id": "eLpeTPBvYxLv",
    "outputId": "5b068b56-ed24-4730-cbae-ec2c8b3cc219"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "frames = []\n",
    "state, info = env.reset()\n",
    "discrete_state = get_discrete_state(state)\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    discrete_state = get_discrete_state(state)\n",
    "\n",
    "env.close()\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.axis('off')\n",
    "im = plt.imshow(frames[0])\n",
    "\n",
    "def animate_func(i):\n",
    "    im.set_array(frames[i])\n",
    "    return [im]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate_func, frames=len(frames), interval=50)\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV9hLih9RNhT"
   },
   "source": [
    "## Part 6: Analysis and Conclusions\n",
    "\n",
    "### 6.1 Key Findings\n",
    "\n",
    "**Learning Process:**\n",
    "1. **Initial Phase (Episodes 0-5000)**: High exploration, random behavior, poor performance\n",
    "2. **Learning Phase (Episodes 5000-15000)**: Agent discovers momentum strategy, rewards improve significantly\n",
    "3. **Convergence Phase (Episodes 15000-25000)**: Performance stabilizes, consistent goal reaching\n",
    "\n",
    "**Why Q-Learning Works:**\n",
    "- The algorithm learns to value state-action pairs that lead to the goal\n",
    "- Through temporal difference learning, it propagates rewards backward through action sequences\n",
    "- The discretized Q-table allows efficient lookup and update of learned values\n",
    "\n",
    "**Challenges Addressed:**\n",
    "- **Continuous State Space**: Solved by discretization into 20×20 grid\n",
    "- **Sparse Rewards**: Overcome through proper discount factor and sufficient training\n",
    "- **Exploration-Exploitation**: Managed through epsilon decay strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0P0hPMVRNhT"
   },
   "source": [
    "### 6.2 Q-Learning Formula Explained\n",
    "\n",
    "The Q-Learning update formula we used:\n",
    "\n",
    "```\n",
    "new_q = (1 - α) * current_q + α * (reward + γ * max_future_q)\n",
    "```\n",
    "\n",
    "**Component Breakdown:**\n",
    "- **(1 - α) * current_q**: Retain some of the old Q-value (stability)\n",
    "- **α * (reward + γ * max_future_q)**: Incorporate new information\n",
    "  - **reward**: Immediate feedback from environment\n",
    "  - **γ * max_future_q**: Discounted future rewards\n",
    "\n",
    "This creates a weighted average between:\n",
    "- What we previously believed (old Q-value)\n",
    "- What we just learned (reward + future value)\n",
    "\n",
    "**Why It Works:**\n",
    "- Gradually adjusts beliefs based on experience\n",
    "- Balances short-term rewards with long-term strategy\n",
    "- Converges to optimal policy given enough training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjkk0K3hRNhT"
   },
   "source": [
    "### 6.3 Hyperparameter Impact\n",
    "\n",
    "**Learning Rate (α = 0.1):**\n",
    "- Moderate value allows steady learning without instability\n",
    "- Too high: unstable, oscillating values\n",
    "- Too low: very slow convergence\n",
    "\n",
    "**Discount Factor (γ = 0.95):**\n",
    "- High value emphasizes long-term rewards\n",
    "- Critical for this problem since goal requires sequence of actions\n",
    "- Lower values would lead to myopic behavior\n",
    "\n",
    "**Epsilon Decay:**\n",
    "- Linear decay from 1.0 to 0.0\n",
    "- Ensures sufficient exploration early on\n",
    "- Transitions to exploitation as agent gains knowledge\n",
    "\n",
    "**Discretization (20×20):**\n",
    "- Balance between granularity and computational efficiency\n",
    "- Finer discretization (e.g., 40×40) could improve performance but requires more training\n",
    "- Coarser discretization (e.g., 10×10) trains faster but less precise control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfrHYLbWRNhT"
   },
   "source": [
    "### 6.4 Real-World Applications\n",
    "\n",
    "**Q-Learning Principles Apply To:**\n",
    "\n",
    "1. **Robotics**: Robot navigation, manipulation tasks\n",
    "2. **Game AI**: Chess, Go, video game agents\n",
    "3. **Resource Management**: Traffic light optimization, power grid control\n",
    "4. **Finance**: Trading strategies, portfolio optimization\n",
    "5. **Healthcare**: Treatment planning, drug dosage optimization\n",
    "\n",
    "**Advantages:**\n",
    "- Model-free: doesn't need environment dynamics\n",
    "- Off-policy: can learn from any experience\n",
    "- Guaranteed convergence to optimal policy\n",
    "\n",
    "**Limitations:**\n",
    "- Requires discretization for continuous spaces\n",
    "- Doesn't scale well to high-dimensional problems\n",
    "- Needs many samples for convergence\n",
    "\n",
    "**Solution for Limitations:**\n",
    "Deep Q-Learning (DQN) uses neural networks instead of tables, enabling high-dimensional continuous state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZZYMrMNRNhT"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This assignment successfully implemented Q-Learning for the Mountain Car problem, demonstrating:\n",
    "\n",
    " **Complete Q-Learning Implementation**: From scratch using gymnasium\n",
    " **Proper State Discretization**: Converting continuous to discrete states\n",
    " **Epsilon-Greedy Exploration**: Balancing exploration and exploitation\n",
    " **Q-Table Updates**: Using temporal difference learning\n",
    " **Performance Analysis**: Comprehensive reward tracking and visualization\n",
    " **Successful Learning**: Agent learns to reach goal consistently\n",
    "\n",
    "**Key Takeaway:**\n",
    "Q-Learning enables agents to learn optimal behavior through trial and error, without requiring a model of the environment. The algorithm's simplicity and effectiveness make it a fundamental technique in reinforcement learning, though modern applications often use deep learning extensions like DQN for more complex problems.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "- sentdex Q-Learning Tutorial Series\n",
    "- Gymnasium Documentation: https://gymnasium.farama.org/\n",
    "- Sutton & Barto, \"Reinforcement Learning: An Introduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
