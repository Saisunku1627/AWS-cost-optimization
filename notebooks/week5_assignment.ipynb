{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4905f89e-c882-4bc4-ae59-87bee772af5d",
      "metadata": {
        "id": "4905f89e-c882-4bc4-ae59-87bee772af5d"
      },
      "source": [
        "### Regis University\n",
        "\n",
        "**MSDS688_X70: Artificial Intelligence**  \n",
        "Master of Science in Data Science Program\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xF2CX6-Lzxwn",
      "metadata": {
        "id": "xF2CX6-Lzxwn"
      },
      "source": [
        "#### Week 5: Building Conversational Bots  \n",
        "*GPU Required*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5926f4a-2789-431f-b4b9-4ffe273152a4",
      "metadata": {
        "id": "f5926f4a-2789-431f-b4b9-4ffe273152a4"
      },
      "source": [
        "## Lecture: Week 5 - Building Conversational Bots with DialoGPT\n",
        "\n",
        "### Overview\n",
        "\n",
        "This week, we focus on building conversational chatbots using **DialoGPT**. DialoGPT is a conversational model developed by Microsoft, based on GPT-2, that is fine-tuned specifically for dialogue generation. In this lecture, we will discuss how DialoGPT works, how it can be fine-tuned for custom conversational tasks, how chatbots function from an engineering perspective, and how to build a chatbot capable of generating human-like responses.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **How Chatbots Work**\n",
        "\n",
        "Chatbots are AI-powered systems designed to simulate human conversations. From a technical perspective, chatbots are built using a combination of machine learning algorithms, natural language processing (NLP), and sometimes rule-based systems. Below is a breakdown of how modern chatbots are engineered:\n",
        "\n",
        "#### Key Components of a Chatbot:\n",
        "- **Natural Language Understanding (NLU)**: This component helps the chatbot understand user inputs. It involves breaking down user queries into structured data, extracting the intent (what the user wants), and identifying entities (specific pieces of information like dates, names, or locations).\n",
        "- **Response Generation**: This can be either rule-based (predefined responses) or based on machine learning models. Models like **DialoGPT** generate responses dynamically by predicting the next words in the conversation.\n",
        "- **Dialogue Management**: This component manages the flow of conversation, deciding how the chatbot should respond based on the context. It ensures that the conversation stays coherent and follows a logical sequence.\n",
        "- **Backend APIs and Integrations**: Chatbots often need to integrate with backend systems (e.g., databases, APIs) to retrieve information or complete tasks like booking a ticket or checking account balances.\n",
        "\n",
        "#### Types of Chatbots:\n",
        "1. **Rule-Based Chatbots**: These rely on predefined rules and responses, often using decision trees or simple pattern matching to guide the conversation.\n",
        "2. **AI-Based Chatbots**: These use machine learning models to generate responses. They are more flexible and can handle a wider range of queries but require large amounts of data for training.\n",
        "3. **Hybrid Chatbots**: These combine rule-based systems with AI, allowing the chatbot to handle simple queries through predefined rules and more complex queries through machine learning.\n",
        "\n",
        "#### Key Challenges in Building Chatbots:\n",
        "- **Understanding Ambiguity**: User inputs can often be ambiguous, making it difficult for the chatbot to understand the exact intent.\n",
        "- **Maintaining Context**: Chatbots need to remember and maintain the context of the conversation across multiple turns.\n",
        "- **Response Coherence**: Generating relevant and coherent responses is challenging, especially in long conversations.\n",
        "- **Scalability**: Chatbots need to handle thousands of interactions simultaneously while maintaining performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **How DialoGPT Works**\n",
        "\n",
        "#### DialoGPT Overview:\n",
        "**DialoGPT** is a variant of GPT-2, fine-tuned for conversational AI tasks. It is designed to generate coherent, context-aware responses in a dialogue format. Like GPT-2, DialoGPT uses a **transformer-based** architecture with a **decoder-only** structure, meaning it predicts the next token in a sequence based on all the preceding tokens.\n",
        "\n",
        "#### Key Concepts:\n",
        "- **Pretraining on Conversations**: DialoGPT is pretrained on large conversational datasets, which allows it to generate contextually relevant responses.\n",
        "- **Unidirectional Decoder**: Similar to GPT-2, DialoGPT generates text left-to-right, predicting the next word based on the conversation so far.\n",
        "- **Self-Attention**: DialoGPT uses self-attention mechanisms to focus on relevant parts of the conversation when generating responses.\n",
        "\n",
        "#### How DialoGPT Generates Dialogue:\n",
        "1. **Input Tokenization**: The conversation is tokenized into smaller units (tokens) and fed into the model.\n",
        "2. **Context-Aware Response Generation**: The model generates the next token based on the entire conversation history. This allows it to maintain context and coherence over multiple turns in a conversation.\n",
        "3. **Text Generation**: The generated tokens are decoded back into human-readable text, forming the chatbot’s response.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Fine-Tuning DialoGPT**\n",
        "\n",
        "Fine-tuning allows us to customize DialoGPT to respond better in specific conversational contexts. This involves training the model on a custom dataset, which may include dialogues from a particular domain (e.g., customer support, therapy, or informal chats).\n",
        "\n",
        "#### Steps for Fine-Tuning DialoGPT:\n",
        "1. **Load Pretrained Model**: Start with a pretrained DialoGPT model (e.g., `microsoft/DialoGPT-medium`).\n",
        "2. **Prepare the Dataset**: Use a dialogue dataset like **DailyDialog**. Tokenize the conversations so they can be fed into the model.\n",
        "3. **Train the Model**: Fine-tune DialoGPT on the dataset by training it to predict the next token in each dialogue sequence. Adjust the learning rate and training parameters to fit the dataset size.\n",
        "4. **Evaluation**: After fine-tuning, evaluate the chatbot’s performance by generating responses and comparing them to the expected dialogue or using metrics like **perplexity**.\n",
        "\n",
        "#### Tokenization and Training:\n",
        "- **Tokenization**: The input conversations are tokenized into sequences of tokens. Since dialogues can vary in length, padding may be necessary to ensure the sequences have the same length.\n",
        "- **Training Objective**: The training process involves predicting the next word in a conversation based on the context provided by the previous words.\n",
        "\n",
        "#### Fine-Tuning Parameters:\n",
        "- **Learning Rate**: Controls how quickly the model adjusts its weights during training.\n",
        "- **Epochs**: The number of times the model goes through the training dataset.\n",
        "- **Batch Size**: The number of samples the model processes before updating its weights.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Building a Chatbot with DialoGPT**\n",
        "\n",
        "Once fine-tuned, DialoGPT can be used to build a fully functional chatbot. The chatbot can generate responses to user inputs based on the conversational context, providing coherent and contextually relevant replies.\n",
        "\n",
        "#### Response Generation:\n",
        "- **Max and Min Length**: Control the length of the chatbot’s responses. Setting a minimum length ensures the responses are not too short, while a maximum length prevents overly long replies.\n",
        "- **Temperature**: Adjusts the randomness of the model’s predictions. Higher temperatures make the chatbot more creative, while lower temperatures make the chatbot more deterministic.\n",
        "- **Top-p (Nucleus Sampling)**: Controls the diversity of the generated responses by sampling from the most probable words until the cumulative probability reaches `p` (e.g., 0.9).\n",
        "\n",
        "#### Interaction with the Chatbot:\n",
        "After fine-tuning, you can start a conversation with the chatbot by feeding it input text. The chatbot will generate responses based on its training and the provided input. The conversation history is stored and fed back into the model, allowing the chatbot to maintain context across multiple dialogue turns.\n",
        "\n",
        "```python\n",
        "# Example of chatting with the fine-tuned model\n",
        "def chat_with_model(model, tokenizer):\n",
        "    chat_history_ids = None\n",
        "    while True:\n",
        "        user_input = input(\"User: \")\n",
        "        new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(device)\n",
        "\n",
        "        # Concatenate the new input with previous chat history\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "        # Generate the response\n",
        "        chat_history_ids = model.generate(\n",
        "            bot_input_ids,\n",
        "            max_length=1000,\n",
        "            min_length=10,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode and print the response\n",
        "        bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "        print(f\"Chatbot: {bot_response}\")\n",
        "```\n",
        "\n",
        "#### Controlling the Chatbot’s Behavior:\n",
        "- **No Repeat N-Gram Size**: Prevents the chatbot from repeating phrases or sentences by ensuring that no n-grams (sequences of words) are repeated within the generated response.\n",
        "- **Attention Masking**: Helps the model focus on relevant tokens in the input sequence when generating responses.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Challenges in Building Conversational Bots**\n",
        "\n",
        "While building conversational bots with models like DialoGPT is powerful, there are several challenges to consider:\n",
        "- **Understanding Ambiguity**: User inputs can often be ambiguous, making it difficult for the chatbot to understand the exact intent.\n",
        "- **Maintaining Context**: Chatbots need to remember and maintain the context of the conversation across multiple turns.\n",
        "- **Response Coherence**: Generating relevant and coherent responses is challenging, especially in long conversations.\n",
        "- **Domain Adaptation**: Fine-tuning the model on specific datasets can help adapt the chatbot to particular domains, but it may still struggle with out-of-domain inputs.\n",
        "- **Ethical Considerations**: Conversational bots can generate inappropriate or biased content. It is important to monitor and filter chatbot outputs for ethical and responsible AI use.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This week’s assignment focuses on fine-tuning **DialoGPT** to create a conversational chatbot capable of generating human-like responses. By fine-tuning the model on a custom dataset, adjusting response generation parameters, and evaluating the chatbot’s performance, you will gain insights into the process of building conversational AI systems. Pay attention to the hyperparameters that control response generation, such as temperature, top-p, and n-gram size, as these will have a significant impact on the chatbot’s behavior.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a959f8e6",
      "metadata": {
        "id": "a959f8e6"
      },
      "source": [
        "## Assignment Part 1: Follow Me – Fine-Tuning DialoGPT with Custom Dataset\n",
        "\n",
        "In this section, you will fine-tune the DialoGPT model using a custom dataset to build a conversational chatbot. You will learn how to prepare and fine-tune a model designed for conversational AI and test its ability to generate human-like responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "jJmekG_29lHK"
      },
      "id": "jJmekG_29lHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5547b5f8",
      "metadata": {
        "id": "5547b5f8"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7103838",
      "metadata": {
        "id": "b7103838"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb5e956",
      "metadata": {
        "id": "afb5e956"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained DialoGPT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef19156e",
      "metadata": {
        "id": "ef19156e"
      },
      "outputs": [],
      "source": [
        "# Set the pad_token to eos_token for padding\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c52dd82",
      "metadata": {
        "id": "3c52dd82"
      },
      "outputs": [],
      "source": [
        "# Load and prepare a conversational dataset for fine-tuning\n",
        "# Load only a small subset of the daily_dialog dataset for faster training\n",
        "dataset = load_dataset(\"blended_skill_talk\", split=\"train[:1%]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset = dataset.select(range(min(48, len(dataset))))\n"
      ],
      "metadata": {
        "id": "clf5Uk41Kgta"
      },
      "id": "clf5Uk41Kgta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd4883b",
      "metadata": {
        "id": "3bd4883b"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    # Extract and join all utterance texts from the dialog\n",
        "    joined_dialogs = [\n",
        "        \" \".join([turn[\"text\"] for turn in dialog])\n",
        "        for dialog in examples[\"dialog\"]\n",
        "    ]\n",
        "    return tokenizer(joined_dialogs, truncation=True, padding=\"max_length\", max_length=512)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use as many as are available (up to 100)\n",
        "small_train_dataset = dataset.select(range(min(100, len(dataset))))\n"
      ],
      "metadata": {
        "id": "1vgGVrjeK50_"
      },
      "id": "1vgGVrjeK50_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05dbfc0",
      "metadata": {
        "id": "c05dbfc0"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer for your model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using the 'context' field\n",
        "def tokenize_function(examples):\n",
        "    joined_contexts = [\" \".join(context) for context in examples[\"context\"]]\n",
        "    return tokenizer(joined_contexts, truncation=True, padding=\"max_length\", max_length=512)\n"
      ],
      "metadata": {
        "id": "K8XOfeg6LApt"
      },
      "id": "K8XOfeg6LApt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization and remove original columns\n",
        "tokenized_dataset = small_train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=small_train_dataset.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "dXsvKhMBLCNm"
      },
      "id": "dXsvKhMBLCNm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8848ff58",
      "metadata": {
        "id": "8848ff58"
      },
      "outputs": [],
      "source": [
        "# Define the data collator to dynamically pad inputs and create labels\n",
        "from transformers import DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c085754b",
      "metadata": {
        "id": "c085754b"
      },
      "outputs": [],
      "source": [
        "# Create data collator for masked language modeling (MLM)\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0551a7f",
      "metadata": {
        "id": "f0551a7f"
      },
      "outputs": [],
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Output directory for checkpoints\n",
        "    num_train_epochs=1,  # Number of epochs for fine-tuning\n",
        "    per_device_train_batch_size=2,  # Batch size per device during training\n",
        "    save_steps=1000,  # Save checkpoint every 1000 steps\n",
        "    save_total_limit=2,  # Limit checkpoints to 2\n",
        "    learning_rate=5e-5,  # Learning rate\n",
        "    logging_dir='./logs',  # Directory for logs\n",
        "    logging_steps=100,  # Log every 100 steps\n",
        "    report_to=\"none\"  # Disable WandB logging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c163efd2",
      "metadata": {
        "id": "c163efd2"
      },
      "outputs": [],
      "source": [
        "# Define the Trainer for fine-tuning the model\n",
        "trainer = Trainer(\n",
        "    model=model,  # The pre-trained model to fine-tune\n",
        "    args=training_args,  # Training arguments defined above\n",
        "    train_dataset=tokenized_dataset,  # The smaller dataset used for training\n",
        "    data_collator=data_collator,  # Use the data collator to create labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e90829",
      "metadata": {
        "id": "b0e90829"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7c08de",
      "metadata": {
        "id": "0e7c08de"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./fine_tuned_chatbot\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_chatbot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c278704",
      "metadata": {
        "id": "4c278704"
      },
      "outputs": [],
      "source": [
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf6848f",
      "metadata": {
        "id": "fcf6848f"
      },
      "outputs": [],
      "source": [
        "def chat_with_model(model, tokenizer):\n",
        "    import torch\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    if tokenizer.eos_token is None:\n",
        "        tokenizer.eos_token = tokenizer.sep_token or tokenizer.pad_token or \"[SEP]\"\n",
        "\n",
        "    chat_history_ids = None\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize user input\n",
        "        new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(device)\n",
        "\n",
        "        # Combine with chat history\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "        # Generate response\n",
        "        chat_history_ids = model.generate(\n",
        "            bot_input_ids,\n",
        "            max_length=1000,\n",
        "            min_length=10,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            attention_mask=torch.ones_like(bot_input_ids),\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "        if not bot_response.strip():\n",
        "            bot_response = \"I don't have a response for that.\"\n",
        "\n",
        "        print(f\"Chatbot: {bot_response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59NRd1qh1Dss",
      "metadata": {
        "id": "59NRd1qh1Dss"
      },
      "outputs": [],
      "source": [
        "# Chat with the fine-tuned model\n",
        "#### TODO: No code changes. Chat with the bot and type exit to end the chat. DO NOT hit stop as your chat history will not be saved. ####\n",
        "chat_with_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd3b1d6",
      "metadata": {
        "id": "bdd3b1d6"
      },
      "source": [
        "## Assignment Part 2: Your Turn – Training Your Own Chatbot with Persona Chat\n",
        "\n",
        "In this section, you will train your own chatbot using the provided framework and dataset. You will explore how adjusting different hyperparameters and training data can influence the quality of the bot's responses. **A framework has been provided, and your job is to complete the TODOs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbd9992d",
      "metadata": {
        "id": "cbd9992d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HJ2nM6T_4I5t",
      "metadata": {
        "id": "HJ2nM6T_4I5t"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained DialoGPT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the pad_token to eos_token for padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "6HL3or8nrX_F"
      },
      "id": "6HL3or8nrX_F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load persona-chat data from the CSV file\n",
        "personality_data = pd.read_csv('personality.csv')"
      ],
      "metadata": {
        "id": "JUDkJ0csrYv3"
      },
      "id": "JUDkJ0csrYv3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a subset of the data\n",
        "personality_data_subset = personality_data.head(100)"
      ],
      "metadata": {
        "id": "w2bh2fCWraDP"
      },
      "id": "w2bh2fCWraDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TODO: Define a function to tokenize each row of the dataset ####\n",
        "def tokenize_function(row):\n",
        "    # HINT: Start by combining the text from the 'Persona' and 'chat' columns\n",
        "    # HINT: Use tokenizer to tokenize the combined text\n",
        "    # HINT: Make sure to handle truncation and padding for consistency in input length\n",
        "    # HINT: Set the maximum length to 512 tokens\n",
        "    pass  # Remove this line when implementing the function\n",
        "\n"
      ],
      "metadata": {
        "id": "V13IGgKzrbkt"
      },
      "id": "V13IGgKzrbkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization to the subset of data and store the results in a list\n",
        "tokenized_inputs = [tokenize_function(row) for _, row in personality_data_subset.iterrows()]\n"
      ],
      "metadata": {
        "id": "Vl6ljRXarcfV"
      },
      "id": "Vl6ljRXarcfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset object compatible with the Trainer\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.input_ids = [data['input_ids'] for data in tokenized_data]\n",
        "        self.attention_mask = [data['attention_mask'] for data in tokenized_data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx]),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx]),\n",
        "            'labels': torch.tensor(self.input_ids[idx])  # Labels are same as input for causal language modeling\n",
        "        }"
      ],
      "metadata": {
        "id": "8Gvf_r1_rdMN"
      },
      "id": "8Gvf_r1_rdMN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset from the tokenized inputs\n",
        "dataset = CustomDataset(tokenized_inputs)"
      ],
      "metadata": {
        "id": "7q2bcL6DreI4"
      },
      "id": "7q2bcL6DreI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We do not want masked language modeling for this task\n",
        ")"
      ],
      "metadata": {
        "id": "WXiuL-7breyW"
      },
      "id": "WXiuL-7breyW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TODO: Set up the training arguments for fine-tuning ####\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=...,  # HINT: Choose a directory to save model checkpoints\n",
        "    num_train_epochs=...,  # HINT: Specify the number of epochs for training\n",
        "    per_device_train_batch_size=...,  # HINT: Define the batch size per device\n",
        "    save_steps=...,  # HINT: Decide how often (in steps) to save model checkpoints\n",
        "    save_total_limit=...,  # HINT: Set a limit on the total number of saved checkpoints\n",
        "    learning_rate=...,  # HINT: Choose an appropriate learning rate\n",
        "    logging_dir=...,  # HINT: Select a directory for logging training progress\n",
        "    logging_steps=...,  # HINT: Define logging frequency in steps\n",
        "    report_to=\"none\"  # Disable WandB logging\n",
        ")"
      ],
      "metadata": {
        "id": "1mn1uoYCrfc1"
      },
      "id": "1mn1uoYCrfc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Trainer for fine-tuning the model\n",
        "trainer = Trainer(\n",
        "    model=model,  # The pre-trained model to fine-tune\n",
        "    args=training_args,  # Training arguments defined above\n",
        "    train_dataset=dataset,  # Custom dataset created from personality data\n",
        "    data_collator=data_collator,  # Use the data collator to create labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "PGqgwQLUrglD"
      },
      "id": "PGqgwQLUrglD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "s956dnFprhsv"
      },
      "id": "s956dnFprhsv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./custom_fine_tuned_chatbot\")\n",
        "tokenizer.save_pretrained(\"./custom_fine_tuned_chatbot\")\n"
      ],
      "metadata": {
        "id": "YO5PMm_9ri-y"
      },
      "id": "YO5PMm_9ri-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "bkUrGOJTri3w"
      },
      "id": "bkUrGOJTri3w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick response function for testing\n",
        "def get_single_response(model, tokenizer, question, persona):\n",
        "    # Combine persona with the question\n",
        "    input_text = persona + \" \" + question\n",
        "\n",
        "    # Tokenize input and create attention mask, move to the same device as the model\n",
        "    inputs = tokenizer(input_text + tokenizer.eos_token, return_tensors='pt', padding=True).to(device)\n",
        "\n",
        "    # Generate response using the same parameters as the interactive chat\n",
        "    output_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],  # Pass attention mask to handle padding correctly\n",
        "        max_length=1000,  # Maximum length of the generated sequence\n",
        "        min_length=10,  # Minimum length to avoid empty responses\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        num_return_sequences=1,  # Return one response\n",
        "        no_repeat_ngram_size=3,  # Avoid repetition\n",
        "        temperature=0.7,  # Control the randomness (1.0 = more random, <1.0 = less random)\n",
        "        top_p=0.9,  # Nucleus sampling to generate more diverse responses\n",
        "        do_sample=True  # Enable sampling\n",
        "    )\n",
        "\n",
        "    # Decode and return response\n",
        "    response = tokenizer.decode(output_ids[:, inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "# Test with a single question and persona\n",
        "persona_example = \"I love cooking and exploring new recipes.\"\n",
        "question_example = \"What is your favorite dish to cook?\"\n",
        "\n",
        "response = get_single_response(model, tokenizer, question_example, persona_example)\n",
        "print(f\"Question: {question_example}\")\n",
        "print(f\"Chatbot: {response}\")\n"
      ],
      "metadata": {
        "id": "ramjwB75rnie"
      },
      "id": "ramjwB75rnie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive loop for chatting with the fine-tuned model, incorporating persona data\n",
        "def chat_with_model(model, tokenizer, personality_data):\n",
        "    # Select a concise persona from the dataset\n",
        "    persona = random.choice(personality_data['Persona'])\n",
        "    print(f\"Chatbot's Persona: {persona}\")\n",
        "\n",
        "    chat_history_ids = None\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Combine persona with user input for a brief context\n",
        "        persona_input = persona + \"\\n\\n\" + user_input\n",
        "\n",
        "        # Tokenize input and move to the same device as the model\n",
        "        new_input_ids = tokenizer.encode(persona_input + tokenizer.eos_token, return_tensors='pt').to(device)\n",
        "\n",
        "        # Use only the last interaction to keep responses short and relevant\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "\n",
        "        #### Optional TODO: Adjust generation parameters for fine-tuning response style and length ####\n",
        "\n",
        "        chat_history_ids = model.generate(\n",
        "          bot_input_ids,\n",
        "          max_new_tokens=30,  # Default: 30 tokens to control response length\n",
        "          min_length=5,  # Default: minimum response length of 5 tokens\n",
        "          pad_token_id=tokenizer.eos_token_id,  # Padding token to handle shorter responses\n",
        "          attention_mask=torch.ones_like(bot_input_ids),  # Attention mask to handle padding\n",
        "          num_return_sequences=1,  # Default: generate 1 response\n",
        "          no_repeat_ngram_size=4,  # Default: limit to prevent repeated phrases\n",
        "          temperature=0.6,  # Default: 0.6 for controlled randomness in response\n",
        "          top_k=50,  # Default: top-k sampling for response diversity\n",
        "          do_sample=True  # Default: enable sampling for natural responses\n",
        "      )\n",
        "\n",
        "        # Decode and take only the first sentence for a concise reply\n",
        "        bot_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "        bot_response = bot_response.split(\". \")[0]  # Take the first sentence only\n",
        "\n",
        "        # Handle empty response case\n",
        "        if not bot_response.strip():\n",
        "            bot_response = \"I'm not sure how to respond to that.\"\n",
        "\n",
        "        print(f\"Chatbot: {bot_response}\")\n"
      ],
      "metadata": {
        "id": "sW1mCCaurtu4"
      },
      "id": "sW1mCCaurtu4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat with the fine-tuned model\n",
        "chat_with_model(model, tokenizer, personality_data)"
      ],
      "metadata": {
        "id": "7ryPSCh9rvZf"
      },
      "id": "7ryPSCh9rvZf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: DialoGPT Personality Fine-Tuning Analysis and Comparison to DailyDialog\n",
        "\n",
        "Now that you've fine-tuned DialoGPT using the **Personality dataset**, reflect on your experience and summarize your findings by addressing the following questions:\n",
        "\n",
        "- **Personality Dataset Insights:**  \n",
        "  How effectively did DialoGPT adopt different personalities? Provide examples of distinct behaviors you observed in chatbot responses.\n",
        "\n",
        "- **Fine-Tuning Challenges:**  \n",
        "  What challenges did you face specifically when fine-tuning DialoGPT on the Personality dataset, compared to observations from the DailyDialog fine-tuning?\n",
        "\n",
        "- **Comparative Analysis:**  \n",
        "  How did responses generated by the Personality-fine-tuned DialoGPT differ from those generated by the DailyDialog-fine-tuned model in terms of coherence, context-awareness, and conversational depth?\n",
        "\n",
        "- **Practical Recommendations:**  \n",
        "  In what conversational contexts would you recommend using personality-driven DialoGPT models instead of general dialogue models (e.g., DailyDialog)? Why?\n",
        "\n",
        "**Action:**  \n",
        "Write a concise summary (1-2 paragraphs) of your observations and insights clearly in a markdown cell below.\n"
      ],
      "metadata": {
        "id": "Z0J8D504ziQt"
      },
      "id": "Z0J8D504ziQt"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}