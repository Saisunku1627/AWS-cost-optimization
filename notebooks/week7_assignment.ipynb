{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6656e656-f555-4729-b3f1-b05332661aff",
      "metadata": {
        "id": "6656e656-f555-4729-b3f1-b05332661aff"
      },
      "source": [
        "### Regis University\n",
        "\n",
        "**MSDS688_X70: Artificial Intelligence**  \n",
        "Master of Science in Data Science Program"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b412e2fd",
      "metadata": {
        "id": "b412e2fd"
      },
      "source": [
        "#### Week 7: Music Generation with GANs  \n",
        "*GPU Required*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc175390-7031-4451-a304-de7edda75c1a",
      "metadata": {
        "id": "cc175390-7031-4451-a304-de7edda75c1a"
      },
      "source": [
        "## Lecture: Week 7 - Audio Generation with GANs\n",
        "\n",
        "### Overview\n",
        "\n",
        "This week, we explore how **Generative Adversarial Networks (GANs)** can be adapted to generate audio. Generating audio presents unique challenges compared to image generation, due to the time-series nature of audio data and the complex patterns of frequency and amplitude it contains. In this lecture, we will cover the core principles of GANs, how they can be modified for audio generation, and the technical aspects of building and training these models.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **GANs for Audio Generation: A Brief Introduction**\n",
        "\n",
        "At its core, the architecture of a **Generative Adversarial Network (GAN)** remains consistent whether it is used for images, audio, or other types of data. A GAN consists of two neural networks: the **generator** and the **discriminator**, which play an adversarial game. However, generating audio poses additional challenges because audio data involves both temporal and frequency components.\n",
        "\n",
        "#### Key Components of a GAN:\n",
        "- **Generator (G)**: This network takes a latent vector (random noise) and generates audio waveforms from it. The goal is to produce audio samples that are indistinguishable from real audio.\n",
        "- **Discriminator (D)**: The discriminator classifies input audio as either real (from the dataset) or fake (generated by the generator). The discriminator helps the generator improve by rejecting poor-quality audio samples.\n",
        "\n",
        "In GANs for audio generation, the input and output data are **time-series**. This introduces unique requirements for how the data is modeled and processed.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Challenges in Audio Generation**\n",
        "\n",
        "Generating high-quality audio is more challenging than generating images because:\n",
        "1. **Temporal Structure**: Audio is inherently sequential, meaning there are dependencies between adjacent time points. The model needs to understand both short-term patterns (e.g., individual sound waves) and long-term structures (e.g., rhythms, melodies, phrases).\n",
        "2. **Frequency Representation**: Audio can be represented in both the **time domain** (as raw waveforms) and the **frequency domain** (as spectrograms). Deciding how to represent the audio is critical to the architecture of the GAN.\n",
        "3. **Dimensionality**: Audio data tends to have high dimensionality, especially when sampled at high rates (e.g., 44.1kHz), meaning a single second of audio could contain tens of thousands of data points.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Time-Domain vs. Frequency-Domain Representation**\n",
        "\n",
        "When building GANs for audio generation, it is important to decide whether to represent the audio data in the **time domain** (raw waveforms) or the **frequency domain** (spectrograms).\n",
        "\n",
        "#### Time-Domain (Raw Waveforms):\n",
        "- **Pros**: Captures the raw temporal structure of the sound directly. The generator creates waveforms that are ready to be played back as audio.\n",
        "- **Cons**: Requires very high precision, especially for high-quality audio, and often needs more complex architectures to generate realistic outputs.\n",
        "- **Applications**: Speech synthesis, simple sounds.\n",
        "\n",
        "#### Frequency-Domain (Spectrograms):\n",
        "- **Pros**: Spectrograms represent the frequency content of the audio over time, which allows the model to focus on spectral features that are important for human perception, such as harmonics and timbre.\n",
        "- **Cons**: Requires an additional step to convert the spectrogram back into audio (usually through an inverse Fourier Transform).\n",
        "- **Applications**: Music generation, environmental sounds, complex audio.\n",
        "\n",
        "Both methods have trade-offs, and the choice depends on the specific task and dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Architecture of Audio GANs**\n",
        "\n",
        "The architecture of a GAN for audio generation closely follows the principles of standard GANs but with some modifications to handle the unique structure of audio data.\n",
        "\n",
        "#### 4.1 Generator Architecture\n",
        "\n",
        "The **generator** is responsible for transforming a random latent vector into a structured audio sequence. The generator typically consists of a series of **upsampling layers** (e.g., transposed convolutions) that progressively increase the temporal resolution of the audio data.\n",
        "\n",
        "##### Key Components:\n",
        "1. **Latent Vector (Input)**: The generator starts with a random noise vector, often drawn from a standard normal distribution. This latent vector serves as the seed for generating the audio sequence.\n",
        "   - In audio GANs, the latent vector might include additional parameters related to pitch, tempo, or other audio features.\n",
        "   \n",
        "2. **Transposed Convolutional Layers**: These layers progressively upsample the latent vector into a waveform or spectrogram. The upsampling increases the temporal resolution, allowing the model to capture finer details in the audio.\n",
        "   - For waveform generation, transposed convolutions generate audio samples directly.\n",
        "   - For spectrogram generation, transposed convolutions produce spectrogram features that must later be converted to waveforms.\n",
        "\n",
        "3. **Activation Functions**: Commonly used activation functions include:\n",
        "   - **ReLU (Rectified Linear Unit)**: Introduces non-linearity, which is important for generating complex patterns in audio.\n",
        "   - **tanh**: Often used in the final layer to ensure that the output values are scaled to the appropriate range for audio data.\n",
        "\n",
        "4. **Output**: The generator produces an audio waveform or spectrogram, which can then be post-processed and converted into a playable format (such as WAV files).\n",
        "\n",
        "#### 4.2 Discriminator Architecture\n",
        "\n",
        "The **discriminator** is a binary classifier that tries to distinguish between real audio (from the dataset) and generated audio (from the generator). The architecture of the discriminator is similar to that of convolutional neural networks (CNNs) used for image classification, but adapted for the time-series nature of audio.\n",
        "\n",
        "##### Key Components:\n",
        "1. **Convolutional Layers**: The discriminator uses **strided convolutions** to downsample the input audio (or spectrogram), extracting features that allow it to distinguish between real and fake data.\n",
        "   - In the time domain, the convolutions operate over the raw waveform.\n",
        "   - In the frequency domain, the convolutions operate over the spectrogram.\n",
        "\n",
        "2. **Leaky ReLU Activation**: In each layer of the discriminator, **Leaky ReLU** is used instead of the standard ReLU to ensure that small gradients are preserved, which helps prevent the network from becoming too confident and \"dead\".\n",
        "\n",
        "3. **Sigmoid Output Layer**: The final output of the discriminator is a single scalar value, representing the probability that the input is real (1) or fake (0). This is achieved using a **sigmoid** activation function.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Loss Functions for Audio GANs**\n",
        "\n",
        "The objective of GAN training is to optimize both the generator and the discriminator using a loss function that reflects how well the generator can fool the discriminator.\n",
        "\n",
        "#### Generator Loss:\n",
        "The generator’s goal is to produce audio that is indistinguishable from real audio. The generator loss is calculated based on the discriminator’s ability to classify the generated audio as fake. The generator tries to **minimize** this loss, effectively trying to \"fool\" the discriminator.\n",
        "\n",
        "#### Discriminator Loss:\n",
        "The discriminator’s goal is to correctly classify real and fake audio. Its loss is the binary cross-entropy between the real and generated data classifications. The discriminator tries to **maximize** this loss, aiming to distinguish real from fake audio as effectively as possible.\n",
        "\n",
        "The adversarial nature of these loss functions helps both models improve over time.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Optimization and Training in Audio GANs**\n",
        "\n",
        "Training audio GANs involves a delicate balance between optimizing the generator and discriminator. The models are trained in an alternating fashion:\n",
        "1. **Training the Discriminator**: The discriminator is trained on batches of real audio and generated audio. Its weights are updated to better classify the audio as real or fake.\n",
        "2. **Training the Generator**: The generator is trained to improve its ability to produce realistic audio that can fool the discriminator. This involves updating the generator's weights to minimize the loss based on the discriminator's feedback.\n",
        "\n",
        "#### Key Hyperparameters:\n",
        "- **Learning Rate**: Both the generator and discriminator are typically trained using the **Adam optimizer** with carefully tuned learning rates to ensure stable convergence.\n",
        "- **Batch Size**: Smaller batch sizes are often used in audio GANs due to the high dimensionality of audio data.\n",
        "- **Latent Dimensionality**: The size of the latent vector controls the variability in the generated audio. Higher-dimensional latent spaces allow for more complex variations in generated audio sequences.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Challenges in Audio GANs**\n",
        "\n",
        "While GANs for image generation are well understood, GANs for audio pose additional challenges:\n",
        "1. **Training Stability**: Like all GANs, audio GANs are prone to instability during training. It is crucial to maintain a careful balance between the discriminator and generator to prevent issues like mode collapse or vanishing gradients.\n",
        "2. **Audio Quality**: High-fidelity audio is difficult to generate, especially when producing long sequences. Fine-tuning the generator to produce smooth, coherent audio over time is non-trivial.\n",
        "3. **Evaluation**: Evaluating the quality of generated audio is subjective and often requires manual listening, though some automatic metrics like **Fréchet Audio Distance (FAD)** have been developed.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This week’s assignment introduces you to the fascinating world of **audio generation with GANs**. You will learn how to design and train GANs for generating audio sequences, either in the time domain (as waveforms) or frequency domain (as spectrograms). Pay close attention to the unique challenges posed by audio data, including the temporal dependencies and high dimensionality, and experiment with different architectures and loss functions to achieve high-quality results.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c485e3f9",
      "metadata": {
        "id": "c485e3f9"
      },
      "source": [
        "## Assignment Part 1: Follow Me – Music Generation Using GANs\n",
        "\n",
        "In this section, we will build and train a Generative Adversarial Network (GAN) to generate music sequences, and we will save the results as MIDI and WAV files. You will explore how GANs can be used to generate creative outputs such as music.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed48be3f",
      "metadata": {
        "id": "ed48be3f"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow wave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "670a7466",
      "metadata": {
        "id": "670a7466"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import wave\n",
        "import struct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21597bb",
      "metadata": {
        "id": "c21597bb"
      },
      "outputs": [],
      "source": [
        "# Define the generator model\n",
        "def build_generator(latent_dim):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(latent_dim,)))  # Replaced input_dim with Input layer\n",
        "    model.add(layers.Dense(256, activation=\"relu\"))\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dense(1024, activation=\"relu\"))\n",
        "    model.add(layers.Dense(128, activation=\"tanh\"))  # Output layer\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9f3860",
      "metadata": {
        "id": "de9f3860"
      },
      "outputs": [],
      "source": [
        "# Define the discriminator model\n",
        "def build_discriminator(input_shape):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Using Input layer instead of input_shape\n",
        "    model.add(layers.Dense(512, activation=\"relu\"))\n",
        "    model.add(layers.Dense(256, activation=\"relu\"))\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))  # Binary classification\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "637c6e83",
      "metadata": {
        "id": "637c6e83"
      },
      "outputs": [],
      "source": [
        "# GAN Model combining generator and discriminator\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during GAN training\n",
        "    gan_input = layers.Input(shape=(latent_dim,))\n",
        "    generated_sequence = generator(gan_input)\n",
        "    gan_output = discriminator(generated_sequence)\n",
        "\n",
        "    gan = tf.keras.models.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17cdf4d5",
      "metadata": {
        "id": "17cdf4d5"
      },
      "outputs": [],
      "source": [
        "import wave\n",
        "import struct\n",
        "\n",
        "# Generate WAV file from the generated sequence (simple sine wave approximation)\n",
        "def sequence_to_wav(sequence, wav_file=\"generated_music.wav\", duration_per_note=0.5, sample_rate=44100):\n",
        "    # Clip values to ensure they're within the MIDI range (0 to 127)\n",
        "    sequence = np.clip((sequence + 1) * 63.5, 0, 127)  # Rescale from [-1, 1] to [0, 127]\n",
        "\n",
        "    # Define basic parameters for audio synthesis\n",
        "    num_samples_per_note = int(sample_rate * duration_per_note)\n",
        "    amplitude = 32767  # Max amplitude for WAV files\n",
        "    data = []\n",
        "\n",
        "    # Generate sine wave for each note in the sequence\n",
        "    for note in sequence:\n",
        "        frequency = 440.0 * (2.0 ** ((note - 69.0) / 12.0))  # Convert MIDI note to frequency\n",
        "        for i in range(num_samples_per_note):\n",
        "            sample = amplitude * np.sin(2 * np.pi * frequency * (i / sample_rate))\n",
        "            data.append(int(sample))\n",
        "\n",
        "    # Write to a WAV file\n",
        "    with wave.open(wav_file, 'w') as wav_out:\n",
        "        wav_out.setnchannels(1)  # Mono sound\n",
        "        wav_out.setsampwidth(2)  # 16-bit sound\n",
        "        wav_out.setframerate(sample_rate)\n",
        "        wav_out.writeframes(b''.join([struct.pack('<h', sample) for sample in data]))  # Pack as signed 16-bit integer\n",
        "\n",
        "    print(f\"Saved generated music to {wav_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165f7050",
      "metadata": {
        "id": "165f7050"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "latent_dim = 100\n",
        "input_shape = (128,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f04fb8",
      "metadata": {
        "id": "89f04fb8"
      },
      "outputs": [],
      "source": [
        "# Build models\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator((128,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03431817",
      "metadata": {
        "id": "03431817"
      },
      "outputs": [],
      "source": [
        "# Compile generator and discriminator separately\n",
        "generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75322441",
      "metadata": {
        "id": "75322441"
      },
      "outputs": [],
      "source": [
        "# Build and compile the GAN model\n",
        "gan = build_gan(generator, discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cd07250",
      "metadata": {
        "id": "4cd07250"
      },
      "outputs": [],
      "source": [
        "# Training loop (simplified)\n",
        "def train_gan(gan, generator, discriminator, epochs, batch_size, latent_dim):\n",
        "    for epoch in range(epochs):\n",
        "        # Generate random noise as input\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        generated_sequences = generator.predict(noise)\n",
        "\n",
        "        # Generate \"real\" sequences (random data for this example)\n",
        "        real_sequences = np.random.rand(batch_size, 128)\n",
        "\n",
        "        # Combine real and generated sequences\n",
        "        combined_sequences = np.concatenate([generated_sequences, real_sequences])\n",
        "\n",
        "        # Create labels for real (1) and fake (0)\n",
        "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator.trainable = True  # Allow discriminator to be trainable\n",
        "        d_loss = discriminator.train_on_batch(combined_sequences, labels)\n",
        "\n",
        "        # Train generator via GAN\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        misleading_labels = np.ones((batch_size, 1))\n",
        "        discriminator.trainable = False  # Freeze discriminator during GAN training\n",
        "        g_loss = gan.train_on_batch(noise, misleading_labels)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
        "\n",
        "    # Generate a final WAV file after all epochs are complete\n",
        "    final_noise = np.random.normal(0, 1, (1, latent_dim))\n",
        "    final_generated_sequence = generator.predict(final_noise)[0]\n",
        "    sequence_to_wav(final_generated_sequence, wav_file=\"walkthrough_generated_music.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca02368f",
      "metadata": {
        "id": "ca02368f"
      },
      "outputs": [],
      "source": [
        "# Train the GAN\n",
        "train_gan(gan, generator, discriminator, epochs=50, batch_size=32, latent_dim=latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the generated WAV file from Colab to your local machine\n",
        "from google.colab import files\n",
        "\n",
        "files.download('walkthrough_generated_music.wav')"
      ],
      "metadata": {
        "id": "3Fl5adjD1N_J"
      },
      "id": "3Fl5adjD1N_J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "42801333",
      "metadata": {
        "id": "42801333"
      },
      "source": [
        "## Assignment Part 2: Fine-Tuning the Music Generation Model\n",
        "\n",
        "In this section, the GAN model will be fine-tuned to create custom music. We provide the rest of the model structure, but you need to update the GAN to explore different configurations and produce unique music. **A framework has been provided, and your job is to complete the TODOs.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FsauYgEV-Tj-",
      "metadata": {
        "id": "FsauYgEV-Tj-"
      },
      "outputs": [],
      "source": [
        "#### TODO: Define your custom hyperparameters ####\n",
        "custom_latent_dim = 64  # Change the latent dimension\n",
        "custom_epochs = 30  # Modify the number of epochs\n",
        "custom_batch_size = 16  # Modify the batch size\n",
        "custom_optimizer = 'rmsprop'  # Try a different optimizer (e.g., 'rmsprop')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2NR5lw6v-pFF",
      "metadata": {
        "id": "2NR5lw6v-pFF"
      },
      "outputs": [],
      "source": [
        "#### TODO: Modify the generator model (example: changing number of layers and units) ####\n",
        "def custom_build_generator(latent_dim):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=(latent_dim,)))  # Input layer\n",
        "\n",
        "    #### TODO ####\n",
        "\n",
        "    model.add(layers.Dense(128, activation=\"tanh\"))  # Output with 128 units to match discriminator input\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FeJGYuHC-q7a",
      "metadata": {
        "id": "FeJGYuHC-q7a"
      },
      "outputs": [],
      "source": [
        "#### TODO: Modify the discriminator model (example: changing number of layers and units) ####\n",
        "def custom_build_discriminator(input_shape):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))  # Input layer\n",
        "\n",
        "    #### TODO ####\n",
        "\n",
        "    model.add(layers.Dense(1, activation=\"sigmoid\"))  # Output for binary classification\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l8gNKa7g-sia",
      "metadata": {
        "id": "l8gNKa7g-sia"
      },
      "outputs": [],
      "source": [
        "# Rebuild the models with the custom parameters\n",
        "custom_generator = custom_build_generator(custom_latent_dim)\n",
        "custom_discriminator = custom_build_discriminator((128,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ACcQjbMa-V4f",
      "metadata": {
        "id": "ACcQjbMa-V4f"
      },
      "outputs": [],
      "source": [
        "# Compile the models with the new optimizer\n",
        "custom_generator.compile(optimizer=custom_optimizer, loss='binary_crossentropy')\n",
        "custom_discriminator.compile(optimizer=custom_optimizer, loss='binary_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cGDpGhLH_ldg",
      "metadata": {
        "id": "cGDpGhLH_ldg"
      },
      "outputs": [],
      "source": [
        "# GAN Model combining generator and discriminator with custom latent dimension\n",
        "def build_gan(generator, discriminator, latent_dim):\n",
        "    discriminator.trainable = False  # Freeze the discriminator during GAN training\n",
        "    gan_input = layers.Input(shape=(latent_dim,))  # Ensure the latent_dim is passed here\n",
        "    generated_sequence = generator(gan_input)\n",
        "    gan_output = discriminator(generated_sequence)\n",
        "\n",
        "    gan = tf.keras.models.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return gan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qz-a-kLW-XEn",
      "metadata": {
        "id": "qz-a-kLW-XEn"
      },
      "outputs": [],
      "source": [
        "# Rebuild the GAN model with the updated generator, discriminator, and latent_dim\n",
        "custom_gan = build_gan(custom_generator, custom_discriminator, custom_latent_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yTw7VYNE-4dA",
      "metadata": {
        "id": "yTw7VYNE-4dA"
      },
      "outputs": [],
      "source": [
        "# Train the customized GAN\n",
        "train_gan(custom_gan, custom_generator, custom_discriminator, epochs=custom_epochs, batch_size=custom_batch_size, latent_dim=custom_latent_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MqXoQLeO-ZOu",
      "metadata": {
        "id": "MqXoQLeO-ZOu"
      },
      "outputs": [],
      "source": [
        "# Save the WAV file from the generated music\n",
        "generated_noise = np.random.normal(0, 1, (1, custom_latent_dim))  # Generate new noise\n",
        "generated_sequence = custom_generator.predict(generated_noise)  # Generate a new sequence\n",
        "sequence_to_wav(generated_sequence[0], wav_file=\"custom_generated_music.wav\")  # Save to a new WAV file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the generated WAV file from Colab to your local machine\n",
        "from google.colab import files\n",
        "\n",
        "files.download('custom_generated_music.wav')"
      ],
      "metadata": {
        "id": "f7-hWQOF05q3"
      },
      "id": "f7-hWQOF05q3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Music Files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wave\n",
        "\n",
        "# Function to read WAV file data\n",
        "def read_wav_data(wav_file):\n",
        "    with wave.open(wav_file, 'rb') as wav:\n",
        "        frames = wav.readframes(wav.getnframes())\n",
        "        waveform = np.frombuffer(frames, dtype=np.int16)  # Assuming 16-bit WAV\n",
        "        return waveform\n",
        "\n",
        "# Load the waveforms of both files\n",
        "custom_wav = \"custom_generated_music.wav\"\n",
        "walkthrough_wav = \"walkthrough_generated_music.wav\"\n",
        "\n",
        "custom_waveform = read_wav_data(custom_wav)\n",
        "walkthrough_waveform = read_wav_data(walkthrough_wav)\n",
        "\n",
        "# Make sure both waveforms are the same length for comparison\n",
        "min_length = min(len(custom_waveform), len(walkthrough_waveform))\n",
        "custom_waveform = custom_waveform[:min_length]\n",
        "walkthrough_waveform = walkthrough_waveform[:min_length]\n",
        "\n",
        "# Calculate the difference between the waveforms\n",
        "waveform_difference = custom_waveform - walkthrough_waveform\n",
        "\n",
        "# Sample the data points to avoid overcrowding (e.g., every 100th point)\n",
        "sampling_rate = 100\n",
        "sampled_custom_waveform = custom_waveform[::sampling_rate]\n",
        "sampled_walkthrough_waveform = walkthrough_waveform[::sampling_rate]\n",
        "sampled_difference = waveform_difference[::sampling_rate]\n",
        "\n",
        "# Plot the waveforms and their difference\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Custom generated waveform\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(sampled_custom_waveform, color='blue', alpha=0.7, linewidth=0.7)\n",
        "plt.title(\"Waveform of Custom Generated Music\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "\n",
        "# Walkthrough generated waveform\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(sampled_walkthrough_waveform, color='green', alpha=0.7, linewidth=0.7)\n",
        "plt.title(\"Waveform of Walkthrough Generated Music\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "\n",
        "# Difference between waveforms\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(sampled_difference, color='red', alpha=0.8, linewidth=0.7)\n",
        "plt.title(\"Difference Between Custom and Walkthrough Waveforms\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Amplitude Difference\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "im1_oE9JHa52"
      },
      "id": "im1_oE9JHa52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Audio Generation with GANs from Random Noise Analysis\n",
        "\n",
        "Now that you've trained a Generative Adversarial Network (GAN) to generate audio sequences purely from random noise, summarize your experience and insights by addressing the following questions:\n",
        "\n",
        "- **Generated Audio Quality:**  \n",
        "  How coherent and realistic was the audio generated entirely from random noise? Describe specific characteristics or limitations you observed.\n",
        "\n",
        "- **Training Insights and Challenges:**  \n",
        "  What challenges or issues arose during the training process (e.g., stability, convergence)? How did you manage or resolve these challenges?\n",
        "\n",
        "- **Impact of Hyperparameters:**  \n",
        "  Which hyperparameters (e.g., latent dimension size, learning rate, training epochs) significantly influenced the quality and characteristics of the generated audio? Explain your observations.\n",
        "\n",
        "- **Practical and Creative Potential:**  \n",
        "  Based on your results, in what practical or creative contexts could audio generated from random noise using GANs be utilized?\n",
        "\n",
        "**Action:**  \n",
        "Write a concise summary (1-2 paragraphs) capturing your observations clearly in a markdown cell below.\n"
      ],
      "metadata": {
        "id": "qCJo8zCY1fUv"
      },
      "id": "qCJo8zCY1fUv"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}