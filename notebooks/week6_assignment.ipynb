{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8f529c6-09ee-4830-a70f-ba4060c93f31",
      "metadata": {
        "id": "b8f529c6-09ee-4830-a70f-ba4060c93f31"
      },
      "source": [
        "### Regis University\n",
        "\n",
        "**MSDS688_X70: Artificial Intelligence**  \n",
        "Master of Science in Data Science Program"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6301e9",
      "metadata": {
        "id": "5f6301e9"
      },
      "source": [
        "#### Week 6: Generative Adversarial Networks (GANs) for Image Generation  \n",
        "*GPU Required*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c839bb30-dece-42cb-b99a-6512f6158376",
      "metadata": {
        "id": "c839bb30-dece-42cb-b99a-6512f6158376"
      },
      "source": [
        "## Lecture: Week 6 - Introduction to Deep Convolutional GANs (DCGAN)\n",
        "\n",
        "### Overview\n",
        "\n",
        "This week, we are diving into **Deep Convolutional Generative Adversarial Networks (DCGANs)**, a specific type of **Generative Adversarial Network (GAN)** that uses convolutional layers for image generation. This lecture will introduce you to the fundamental concepts of GANs, followed by a deep, detailed exploration of the architecture of DCGANs, including the generator and discriminator networks, how they function, and how they work together.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **What is a GAN?**\n",
        "\n",
        "A **Generative Adversarial Network (GAN)** consists of two neural networks — a **generator** and a **discriminator** — that compete against each other in a game-theoretic scenario. The goal of the generator is to create data (e.g., images) that can fool the discriminator into thinking the data is real, while the discriminator aims to correctly classify whether the data it receives is real (from the dataset) or fake (from the generator).\n",
        "\n",
        "#### Key Elements:\n",
        "- **Generator (G)**: Takes a random noise vector and transforms it into a data point, like an image.\n",
        "- **Discriminator (D)**: A classifier that tries to distinguish between real data and generated data.\n",
        "- **Latent Space**: The generator starts with a random input vector sampled from a probability distribution (usually Gaussian), referred to as **latent space**.\n",
        "\n",
        "In essence, GANs work by having the two networks train each other in an adversarial process, pushing the generator to create better and more realistic images over time.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Why DCGAN?**\n",
        "\n",
        "While traditional GANs can generate data using fully connected layers, they struggle to capture the intricate structures in image data. **DCGANs** address this by using **convolutional layers**, which are better suited for processing image data because of their ability to recognize spatial hierarchies, like edges, textures, and higher-level patterns.\n",
        "\n",
        "DCGAN introduces several architectural enhancements specifically designed for generating images:\n",
        "- **Convolutional Layers**: Both the generator and discriminator are based on convolutional layers, allowing them to better capture and process image features.\n",
        "- **Transposed Convolutions**: The generator uses transposed convolutions (also known as deconvolutions) to upsample latent vectors into images.\n",
        "- **Strided Convolutions**: The discriminator uses strided convolutions to downsample images and extract relevant features for classification.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **DCGAN Architecture Breakdown**\n",
        "\n",
        "The architecture of a DCGAN is composed of two networks:\n",
        "1. **Generator Network**: Responsible for creating images.\n",
        "2. **Discriminator Network**: Responsible for evaluating the quality of the generated images.\n",
        "\n",
        "#### 3.1 Generator Architecture\n",
        "\n",
        "The **generator** transforms a randomly sampled vector (latent vector) into an image. The core idea behind the generator is to progressively **upsample** the input latent vector through **transposed convolutional layers** until it becomes a full-sized image.\n",
        "\n",
        "##### Key Components of the Generator:\n",
        "1. **Latent Vector (Input)**: The generator starts with a small latent vector, typically sampled from a standard normal distribution (e.g., a 100-dimensional vector sampled from \\( N(0, 1) \\)).\n",
        "   - **Why random noise?**: The latent vector is the seed for generating the image. Starting from a random vector allows the generator to explore many possible outputs.\n",
        "\n",
        "2. **Transposed Convolutions**: These layers reverse the downsampling process of normal convolutions. In traditional convolutions, you reduce the image size, but in **transposed convolutions**, the goal is to **increase** the spatial dimensions, i.e., upsample the feature map back into an image. These layers learn how to construct higher resolution features from the noise vector.\n",
        "   - **Filter size**: Controls how much detail is added at each layer.\n",
        "   - **Stride**: Controls the step size for upsampling, typically a stride of 2 to double the spatial resolution.\n",
        "   \n",
        "3. **Batch Normalization**: Each transposed convolutional layer is followed by a **batch normalization** layer. Batch normalization stabilizes the training by normalizing the activations and helps prevent internal covariate shift (large swings in the distribution of activations during training). This speeds up the learning process and allows the network to train with higher learning rates.\n",
        "   - **Why Batch Norm?**: It ensures that the inputs to each layer maintain a stable distribution, which is important for maintaining training stability in GANs.\n",
        "\n",
        "4. **Activation Functions**:\n",
        "   - **ReLU Activation**: In most layers of the generator, the **ReLU** activation function is used. ReLU (Rectified Linear Unit) is commonly used in deep learning because it introduces non-linearity without suffering from vanishing gradients.\n",
        "   - **tanh Activation**: The final layer of the generator uses the **tanh** activation function to output pixel values. This ensures that the pixel values are scaled between -1 and 1, which is typical for image normalization.\n",
        "\n",
        "##### Generator Architecture Summary:\n",
        "- **Input**: Latent vector (e.g., 100-dimensional).\n",
        "- **Transposed Conv Layer 1**: Upsampling the latent vector (noise) and applying ReLU activation.\n",
        "- **Batch Normalization**: To stabilize training.\n",
        "- **Transposed Conv Layer 2**: Further upsampling with ReLU and Batch Norm.\n",
        "- **Final Transposed Conv Layer**: Producing the image output with **tanh** activation.\n",
        "\n",
        "By the end of this process, the generator has transformed a random noise vector into a high-dimensional image (e.g., 64x64x3 for a color image).\n",
        "\n",
        "#### 3.2 Discriminator Architecture\n",
        "\n",
        "The **discriminator** is a **binary classifier** that takes an image as input (either a real image from the dataset or a fake image from the generator) and outputs a single value indicating whether the input is real or fake.\n",
        "\n",
        "##### Key Components of the Discriminator:\n",
        "1. **Input**: An image, either from the dataset or generated by the generator.\n",
        "   - The input is a 2D image with three color channels (RGB) or a grayscale image.\n",
        "   \n",
        "2. **Convolutional Layers**: The discriminator uses standard **convolutional layers** to progressively **downsample** the input image. As the input passes through these layers, the spatial resolution is reduced while the depth (number of feature maps) increases, allowing the network to learn increasingly complex features of the image, like textures and shapes.\n",
        "   - **Strided Convolutions**: The discriminator uses **strided convolutions** to downsample the image, similar to max-pooling but with better learnable parameters.\n",
        "   \n",
        "3. **Leaky ReLU Activation**: Instead of the standard ReLU used in the generator, the discriminator uses **Leaky ReLU** activation. This variant of ReLU allows small negative gradients to pass through, preventing dead neurons and helping the model learn more robust features.\n",
        "   - **Why Leaky ReLU?**: In the early stages of training, the discriminator needs to learn from both real and generated images, and using Leaky ReLU helps propagate gradients even when the activation is negative, improving training dynamics.\n",
        "\n",
        "4. **Batch Normalization**: Similar to the generator, **batch normalization** is applied after each convolutional layer to stabilize the training and maintain a stable gradient flow.\n",
        "   \n",
        "5. **Final Layer**: The final convolutional layer is followed by a **sigmoid activation function**, which outputs a single scalar value between 0 and 1. This value represents the discriminator’s confidence that the input image is real (1) or fake (0).\n",
        "\n",
        "##### Discriminator Architecture Summary:\n",
        "- **Input**: Image (e.g., 64x64x3).\n",
        "- **Conv Layer 1**: Downsampling with strided convolution and Leaky ReLU activation.\n",
        "- **Batch Normalization**: To stabilize training.\n",
        "- **Conv Layer 2**: Further downsampling with Leaky ReLU and Batch Norm.\n",
        "- **Final Conv Layer**: Outputs a scalar value with sigmoid activation to classify real or fake.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **DCGAN Training Process**\n",
        "\n",
        "The training process for a DCGAN involves the generator and discriminator competing against each other in a **minimax game**. The objective is to optimize both networks to achieve their respective goals:\n",
        "- The generator wants to minimize the loss by generating images that can fool the discriminator.\n",
        "- The discriminator wants to maximize the loss by correctly classifying real vs. fake images.\n",
        "\n",
        "#### Key Steps in Training:\n",
        "1. **Training the Discriminator**: In each iteration, the discriminator is trained to maximize the probability of assigning the correct label (real/fake) to real and generated images. The discriminator gets two types of input:\n",
        "   - Real images from the dataset.\n",
        "   - Fake images from the generator.\n",
        "   The discriminator is updated to become better at distinguishing real from fake images.\n",
        "\n",
        "2. **Training the Generator**: After the discriminator is updated, the generator is trained. The generator aims to minimize the probability that the discriminator will correctly classify its output as fake. In other words, the generator tries to make the discriminator \"believe\" its fake images are real.\n",
        "\n",
        "3. **Adversarial Process**: Both networks train simultaneously. The generator constantly improves to fool the discriminator, while the discriminator improves to better detect the generator’s fakes. This adversarial dynamic leads to increasingly realistic images as training progresses.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Challenges in Training DCGANs**\n",
        "\n",
        "Training DCGANs can be challenging due to the instability inherent in adversarial training. Common issues include:\n",
        "- **Mode Collapse**: The generator starts producing very similar images, limiting diversity. This happens when the generator converges to a narrow solution where it produces only a subset of possible outputs.\n",
        "- **Vanishing Gradients**: When the discriminator becomes too confident and the generator produces poor results, the gradients passed back to the generator can become very small, making it difficult for the generator to learn.\n",
        "- **Balancing Training**: It’s crucial to maintain a balance between the discriminator and the generator. If one network becomes significantly better than the other, training can stagnate.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This week’s assignment introduces you to the architecture and training process of **Deep Convolutional GANs (DCGANs)**. You will build and train a DCGAN, exploring how the generator and discriminator work together in an adversarial setting to produce realistic images. Pay close attention to the structure of both networks and experiment with hyperparameters like learning rates and batch normalization to see their impact on performance.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b3c69d",
      "metadata": {
        "id": "38b3c69d"
      },
      "source": [
        "## Assignment Part 1: Follow Me – DCGAN for Image Generation using MNIST\n",
        "\n",
        "In this section, we will build and train a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from the MNIST dataset. We will visualize the generated images and explore the architecture of the model, focusing on how GANs work in generating realistic images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F3BILiuu5cpZ",
      "metadata": {
        "id": "F3BILiuu5cpZ"
      },
      "outputs": [],
      "source": [
        "!pip install torch matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0643cbd",
      "metadata": {
        "id": "d0643cbd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39af301e",
      "metadata": {
        "id": "39af301e"
      },
      "outputs": [],
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad01800",
      "metadata": {
        "id": "8ad01800",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Create a directory for saving generated images\n",
        "os.makedirs(\"generated_images\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b2363b",
      "metadata": {
        "id": "f9b2363b",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# ---------------- DCGAN for MNIST ----------------\n",
        "class QuickMNISTGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuickMNISTGenerator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 128, 7, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),  # 1 channel for grayscale\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a677f50",
      "metadata": {
        "id": "0a677f50",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Discriminator for MNIST\n",
        "class QuickMNISTDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuickMNISTDiscriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 1, 7, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c2f343",
      "metadata": {
        "id": "20c2f343"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "latent_dim = 100\n",
        "mnist_gen = QuickMNISTGenerator().to(device)\n",
        "mnist_disc = QuickMNISTDiscriminator().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "772f27c3",
      "metadata": {
        "id": "772f27c3"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(mnist_gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(mnist_disc.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fdf0580",
      "metadata": {
        "id": "6fdf0580",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset for training\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
        "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756ef7ee",
      "metadata": {
        "id": "756ef7ee",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Training function for the DCGAN\n",
        "def train_dcgan(dataloader, generator, discriminator, optimizer_g, optimizer_d, criterion, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (imgs, _) in enumerate(dataloader):\n",
        "            # Update Discriminator\n",
        "            optimizer_d.zero_grad()\n",
        "            real_imgs = imgs.to(device)\n",
        "            real_labels = torch.ones(imgs.size(0), 1).to(device)\n",
        "            fake_labels = torch.zeros(imgs.size(0), 1).to(device)\n",
        "\n",
        "            outputs = discriminator(real_imgs)\n",
        "            outputs = outputs.view(real_imgs.size(0), -1).mean(1).unsqueeze(1)  # Flatten output to match label size\n",
        "            loss_real = criterion(outputs, real_labels)\n",
        "\n",
        "            z = torch.randn(imgs.size(0), latent_dim, 1, 1).to(device)\n",
        "            fake_imgs = generator(z)\n",
        "            outputs = discriminator(fake_imgs.detach())\n",
        "            outputs = outputs.view(fake_imgs.size(0), -1).mean(1).unsqueeze(1)  # Flatten output to match label size\n",
        "            loss_fake = criterion(outputs, fake_labels)\n",
        "\n",
        "            loss_d = loss_real + loss_fake\n",
        "            loss_d.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # Update Generator\n",
        "            optimizer_g.zero_grad()\n",
        "            outputs = discriminator(fake_imgs)\n",
        "            outputs = outputs.view(fake_imgs.size(0), -1).mean(1).unsqueeze(1)  # Flatten output to match label size\n",
        "            loss_g = criterion(outputs, real_labels)\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss D: {loss_d.item()}, Loss G: {loss_g.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d82bf17",
      "metadata": {
        "id": "3d82bf17",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Train the GAN with only 5 epochs for faster results\n",
        "train_dcgan(dataloader, mnist_gen, mnist_disc, optimizer_g, optimizer_d, criterion, num_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffb283f",
      "metadata": {
        "id": "9ffb283f",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Generate and save images to folder\n",
        "def generate_images(generator, num_images=5):\n",
        "    generator.eval()\n",
        "    z = torch.randn(num_images, latent_dim, 1, 1).to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_imgs = generator(z).cpu().numpy()\n",
        "    generator.train()\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img = np.transpose(generated_imgs[i], (1, 2, 0))\n",
        "        img = (img + 1) / 2.0  # Rescale to [0, 1]\n",
        "        plt.imsave(f\"generated_images/generated_image_{i}.png\", img.squeeze(), cmap=\"gray\")\n",
        "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5367c6b7",
      "metadata": {
        "id": "5367c6b7"
      },
      "outputs": [],
      "source": [
        "generate_images(mnist_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab1ba3a",
      "metadata": {
        "id": "bab1ba3a"
      },
      "source": [
        "## Assignment Part 2: Your Turn – Build Your Own DCGAN with Custom Dataset\n",
        "\n",
        "In this section, you'll use your own dataset and design your own DCGAN model to generate images. Follow the tasks below, which include hints for customizing the architecture and dataset. **A framework has been provided, and your job is to complete the TODOs.**\n",
        "\n",
        "You can use any image dataset of your choice (e.g., CIFAR-10, CelebA, or a custom dataset of images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb8f9f4",
      "metadata": {
        "id": "cdb8f9f4"
      },
      "outputs": [],
      "source": [
        "transform_custom = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f60f48b",
      "metadata": {
        "id": "9f60f48b",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "#### TODO: Use the CIFAR-10 dataset or replace it with your own dataset. Either option is fine. ####\n",
        "\n",
        "# Example: CIFAR-10 dataset (you can use this or replace it with your own dataset)\n",
        "custom_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_custom)\n",
        "custom_dataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab30000",
      "metadata": {
        "id": "8ab30000"
      },
      "source": [
        "Task 2: Modify the Generator and Discriminator.\n",
        "\n",
        "Below is the same Generator and Discriminator as used for MNIST.\n",
        "\n",
        "If you're using a dataset with 3-channel RGB images (like CIFAR-10), make sure to adjust the number of input/output channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2c4c10",
      "metadata": {
        "id": "2f2c4c10",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "class CustomGenerator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomGenerator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            #### TODO: Create Model ####\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c3863f",
      "metadata": {
        "id": "55c3863f",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "class CustomDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomDiscriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            #### TODO: Create Model ####\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9f1843",
      "metadata": {
        "id": "ac9f1843"
      },
      "outputs": [],
      "source": [
        "custom_gen = CustomGenerator().to(device)\n",
        "custom_disc = CustomDiscriminator().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d709d85d",
      "metadata": {
        "id": "d709d85d"
      },
      "outputs": [],
      "source": [
        "optimizer_g_custom = optim.Adam(custom_gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d_custom = optim.Adam(custom_disc.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QpOjnAxP7K_F",
      "metadata": {
        "id": "QpOjnAxP7K_F"
      },
      "outputs": [],
      "source": [
        "# Train your DCGAN with your dataset\n",
        "train_dcgan(custom_dataloader, custom_gen, custom_disc, optimizer_g_custom, optimizer_d_custom, criterion, num_epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GZxbeBGD7MfR",
      "metadata": {
        "id": "GZxbeBGD7MfR"
      },
      "outputs": [],
      "source": [
        "generate_images(custom_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: DCGAN Custom Dataset Analysis\n",
        "\n",
        "Now that you've trained a Deep Convolutional Generative Adversarial Network (DCGAN) using your custom dataset, summarize your experience and insights by addressing the following questions:\n",
        "\n",
        "- **Dataset Insights:**  \n",
        "  How effectively did DCGAN generate realistic images from your chosen dataset? Discuss specific visual characteristics and the quality of generated images.\n",
        "\n",
        "- **Training Observations:**  \n",
        "  Did your dataset present any specific challenges during training (e.g., instability, mode collapse, convergence issues)? If so, how did you address these challenges?\n",
        "\n",
        "- **Hyperparameter Impact:**  \n",
        "  How did adjusting hyperparameters (e.g., learning rate, epochs, batch size) influence your model’s performance and generated image quality?\n",
        "\n",
        "- **Practical Applications:**  \n",
        "  Based on your results, what potential real-world applications can you envision for using DCGAN with datasets similar to yours?\n",
        "\n",
        "**Action:**  \n",
        "Write a concise summary (1-2 paragraphs) capturing your observations and insights clearly in a markdown cell below.\n"
      ],
      "metadata": {
        "id": "CSLg5gRM0S6_"
      },
      "id": "CSLg5gRM0S6_"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}