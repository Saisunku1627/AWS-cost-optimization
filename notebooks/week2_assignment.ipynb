{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a4a624e1-a7d8-443c-a709-8613d3b6a75e",
      "metadata": {
        "id": "a4a624e1-a7d8-443c-a709-8613d3b6a75e"
      },
      "source": [
        "### Regis University\n",
        "\n",
        "**MSDS688_X70: Artificial Intelligence**  \n",
        "Master of Science in Data Science Program\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ea8c27b",
      "metadata": {
        "id": "8ea8c27b"
      },
      "source": [
        "#### Week 2: Reinforcement Learning for AI Agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aee6ca5-8738-456a-bb0a-d9e6f85da675",
      "metadata": {
        "id": "2aee6ca5-8738-456a-bb0a-d9e6f85da675"
      },
      "source": [
        "## Lecture: Week 2 - Epsilon Decay and Reinforcement Learning Strategies\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this weekâ€™s assignment, we will focus on an important concept in **reinforcement learning (RL)**: **Epsilon Decay**. This is especially relevant for exploration in RL algorithms like **Q-Learning** and **Deep Q Networks (DQN)**. The goal is to understand the role of epsilon decay in managing the exploration-exploitation trade-off.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Introduction to Reinforcement Learning**\n",
        "\n",
        "Reinforcement learning is a type of machine learning where an **agent** learns by interacting with an environment. The agent takes actions to maximize cumulative rewards. It involves the following key components:\n",
        "- **Agent**: The learner or decision maker.\n",
        "- **Environment**: The world through which the agent interacts.\n",
        "- **State (s)**: The current situation of the agent.\n",
        "- **Action (a)**: The decisions the agent makes.\n",
        "- **Reward (r)**: The feedback from the environment after an action.\n",
        "\n",
        "The agent aims to learn an **optimal policy** that maximizes the total reward over time by balancing two conflicting strategies:\n",
        "- **Exploration**: Trying new actions to discover more information about the environment.\n",
        "- **Exploitation**: Using known actions that give the highest reward based on the current knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Epsilon-Greedy Strategy**\n",
        "\n",
        "In RL, one common approach to balance exploration and exploitation is the **epsilon-greedy** strategy:\n",
        "- With probability **epsilon** (ðœ–), the agent chooses a random action (exploration).\n",
        "- With probability **1 - epsilon**, the agent chooses the action that maximizes the known reward (exploitation).\n",
        "\n",
        "#### How Epsilon Decay Works:\n",
        "- **Initial Exploration**: At the start, **epsilon** is usually set to a high value (close to 1), so the agent explores a lot of different actions.\n",
        "- **Decay Over Time**: As the agent learns more about the environment, **epsilon** gradually decreases, encouraging the agent to exploit the best-known actions more frequently.\n",
        "\n",
        "#### Diagram: Epsilon Decay\n",
        "\n",
        "```\n",
        "Initial Epsilon (ðœ– = 1)               Final Epsilon (ðœ– = 0.1)\n",
        "-------------------|--------------------------------------|-->\n",
        "   Exploration                       Exploitation\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Epsilon Decay Formula**\n",
        "\n",
        "A common method to control the decay of epsilon over episodes is using the **exponential decay function**:\n",
        "\n",
        "epsilon = epsilon_min + (epsilon_max - epsilon_min) * exp(-lambda * episode)\n",
        "\n",
        "Where:\n",
        "\n",
        "- epsilon_min: The minimum value epsilon will reach after sufficient episodes.\n",
        "- epsilon_max: The initial value of epsilon at the beginning of training.\n",
        "- lambda: The decay rate that controls how fast epsilon decreases.\n",
        "\n",
        "\n",
        "#### Key Parameters:\n",
        "- **Episode**: A single run of the agent through the environment.\n",
        "- **Epsilon**: The exploration probability.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Exploration vs. Exploitation Trade-Off**\n",
        "\n",
        "The **exploration-exploitation trade-off** is central to reinforcement learning:\n",
        "- **Exploration**: Helps the agent discover new, potentially better actions.\n",
        "- **Exploitation**: Allows the agent to leverage what it has already learned to maximize rewards.\n",
        "\n",
        "Balancing these two is critical:\n",
        "- **Too much exploration**: The agent may waste time on suboptimal actions.\n",
        "- **Too much exploitation**: The agent may get stuck in local optima and fail to discover better strategies.\n",
        "\n",
        "Epsilon decay helps the agent gradually shift from exploration (early in training) to exploitation (as the policy becomes more refined).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **CartPole Environment**\n",
        "Before diving into the **MountainCar** environment, let's explore the **CartPole** environment, another classic problem in reinforcement learning. In CartPole:\n",
        "\n",
        "- The agent controls a cart that can move left or right on a one-dimensional track.\n",
        "- A pole is attached to the cart by an un-actuated joint. The goal is to prevent the pole from falling over by balancing it upright.\n",
        "- The agent receives a reward for each time step that the pole remains upright.\n",
        "\n",
        "CartPole is an excellent environment for understanding basic reinforcement learning concepts because it provides quick feedback and demonstrates the importance of balancing exploration and exploitation. Using the epsilon-greedy strategy with epsilon decay can significantly impact the agent's ability to learn a successful balancing policy.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **MountainCar Environment**\n",
        "\n",
        "In this assignment, you are working with the **MountainCar** environment, which is a standard problem in reinforcement learning. In this environment:\n",
        "- The goal is for the agent (a car) to reach the top of a hill.\n",
        "- The agent must learn to balance momentum and use exploration to figure out how to build enough velocity to climb the hill.\n",
        "\n",
        "The challenge in MountainCar is that the agent must explore enough actions (like driving backward before moving forward) to find an optimal strategy. This makes **epsilon-greedy** and **epsilon decay** crucial for solving the problem efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Visualizing Epsilon Decay**\n",
        "\n",
        "In this assignment, you will visualize how epsilon decreases over episodes. The graph provides insight into how quickly the agent transitions from exploration to exploitation.\n",
        "\n",
        "#### Example Graph:\n",
        "\n",
        "```\n",
        "     Epsilon Decay Over Episodes\n",
        "      +-----------------------------------------+\n",
        "      |                                         |\n",
        "      |              __________                 |\n",
        "      |             /          \\                |\n",
        "      |            /            \\               |\n",
        "      |___________/              \\______________|\n",
        "      |                                         |\n",
        "      +-----------------------------------------+\n",
        "      Episodes ---->                     Epsilon Value\n",
        "```\n",
        "\n",
        "You will implement the **epsilon decay function** and track the epsilon values over time, plotting the resulting curve.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This weekâ€™s assignment focuses on understanding how **epsilon decay** affects learning in reinforcement learning. The exploration-exploitation trade-off is fundamental to making efficient decisions in an uncertain environment. By tuning the decay rate and monitoring epsilon over episodes, we can achieve a balance between discovering new actions and exploiting known good strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b30cd9",
      "metadata": {
        "id": "d4b30cd9"
      },
      "source": [
        "## Assignment Part 1: Follow Me â€“ Q-learning in CartPole\n",
        "\n",
        "In this section, you will apply Q-learning to the CartPole environment and visualize the total rewards over episodes. The goal is to solve the classic control problem where a cart must balance a pole upright using reinforcement learning. Through Q-learning, the agent will learn to take appropriate actions to keep the pole balanced for as long as possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ShT8QeAatLyu",
      "metadata": {
        "id": "ShT8QeAatLyu"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff5b3cf",
      "metadata": {
        "id": "eff5b3cf"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from IPython.display import display, clear_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782740f6",
      "metadata": {
        "id": "782740f6"
      },
      "outputs": [],
      "source": [
        "# Create the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "state = env.reset()[0]  # Reset returns a tuple with (observation, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20241c1",
      "metadata": {
        "id": "c20241c1"
      },
      "outputs": [],
      "source": [
        "# Discretization parameters: one bucket for each of the four dimensions\n",
        "num_buckets = (6, 6, 12, 12)  # (cart position, cart velocity, pole angle, pole velocity at tip)\n",
        "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d749211",
      "metadata": {
        "id": "1d749211"
      },
      "outputs": [],
      "source": [
        "# Clip the bounds to avoid infinite values for CartPole\n",
        "state_bounds[1] = [-4.8, 4.8]  # Cart velocity bounds\n",
        "state_bounds[3] = [-math.radians(50), math.radians(50)]  # Pole velocity bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1035af45",
      "metadata": {
        "id": "1035af45"
      },
      "outputs": [],
      "source": [
        "# Q-table size: depends on discretization\n",
        "q_table = np.zeros(num_buckets + (env.action_space.n,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8de909",
      "metadata": {
        "id": "0a8de909"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "episodes = 1000  # Number of episodes to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a633dc",
      "metadata": {
        "id": "c2a633dc",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "episode_rewards = []\n",
        "epsilon_values = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751cde73",
      "metadata": {
        "id": "751cde73",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Function to discretize the continuous state\n",
        "def discretize_state(state):\n",
        "    discrete_state = []\n",
        "    for i in range(len(state)):\n",
        "        scaling = (state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0])\n",
        "        new_state = int(scaling * num_buckets[i])\n",
        "        new_state = min(num_buckets[i] - 1, max(0, new_state))  # Clip state within bucket range\n",
        "        discrete_state.append(new_state)\n",
        "    return tuple(discrete_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0b9e7b",
      "metadata": {
        "id": "3b0b9e7b"
      },
      "outputs": [],
      "source": [
        "# Q-Learning Algorithm for CartPole\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()[0]  # Grab the observation from reset\n",
        "    state = discretize_state(state)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit learned values\n",
        "\n",
        "        # Take action, observe new state, reward, done (terminated), truncated, and info\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        next_state = discretize_state(next_state)  # Discretize next state\n",
        "\n",
        "        # Update Q-table\n",
        "        q_table[state][action] = q_table[state][action] + alpha * (\n",
        "                reward + gamma * np.max(q_table[next_state]) - q_table[state][action])\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Check if episode is done or truncated\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    # Append total reward for each episode\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    # Append current epsilon value\n",
        "    epsilon_values.append(epsilon)\n",
        "\n",
        "    # Decay epsilon to reduce exploration over time\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "    # Print results for every 100 episodes\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode} - Total Reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee0b135",
      "metadata": {
        "id": "dee0b135"
      },
      "outputs": [],
      "source": [
        "print(\"Training completed for CartPole!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3146570",
      "metadata": {
        "id": "d3146570"
      },
      "source": [
        "Visualization of the results for CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aef3d66",
      "metadata": {
        "id": "4aef3d66"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167bdc5f",
      "metadata": {
        "id": "167bdc5f"
      },
      "outputs": [],
      "source": [
        "# Total Reward per Episode\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_rewards, color='blue')\n",
        "plt.title('Total Reward per Episode (CartPole)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb64d37",
      "metadata": {
        "id": "ccb64d37"
      },
      "outputs": [],
      "source": [
        "# Epsilon Decay over Episodes\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epsilon_values, color='green')\n",
        "plt.title('Epsilon Decay over Episodes (CartPole)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon Value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60af41a9",
      "metadata": {
        "id": "60af41a9"
      },
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eddcba7",
      "metadata": {
        "id": "8eddcba7"
      },
      "source": [
        "## Assignment Part 2: Your Turn â€“ Q-learning in MountainCar\n",
        "In this section, you will apply Q-learning to the MountainCar-v0 environment and visualize the total rewards over episodes. The goal is to solve the classic control problem where a car must navigate up a steep hill using reinforcement learning. **A framework has been provided, and your job is to complete the TODOs.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio"
      ],
      "metadata": {
        "id": "6twaAgRMrl0f"
      },
      "id": "6twaAgRMrl0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f93eac",
      "metadata": {
        "id": "42f93eac"
      },
      "outputs": [],
      "source": [
        "# Create the MountainCar environment\n",
        "env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
        "state = env.reset()[0]  # Reset returns a tuple with (observation, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afee17ab",
      "metadata": {
        "id": "afee17ab"
      },
      "outputs": [],
      "source": [
        "# Discretization parameters for MountainCar\n",
        "num_buckets = (18, 14)  # (car position, car velocity)\n",
        "state_bounds = list(zip(env.observation_space.low, env.observation_space.high))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97875757",
      "metadata": {
        "id": "97875757"
      },
      "outputs": [],
      "source": [
        "# Q-table size: depends on discretization\n",
        "q_table = np.zeros(num_buckets + (env.action_space.n,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eccd0ccd",
      "metadata": {
        "id": "eccd0ccd"
      },
      "outputs": [],
      "source": [
        "# Adjust hyperparameters for MountainCar\n",
        "##### TODO: Customize alpha, gamma, and epsilon for experimentation ####\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995  # Decay rate for exploration\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "episodes = 1000  # Number of episodes to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02fdf94",
      "metadata": {
        "id": "e02fdf94",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "episode_rewards = []\n",
        "epsilon_values = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83a504b",
      "metadata": {
        "id": "c83a504b",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Function to discretize the continuous state for MountainCar\n",
        "def discretize_state(state):\n",
        "    discrete_state = []\n",
        "    for i in range(len(state)):\n",
        "        scaling = (state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0])\n",
        "        new_state = int(scaling * num_buckets[i])\n",
        "        new_state = min(num_buckets[i] - 1, max(0, new_state))  # Clip state within bucket range\n",
        "        discrete_state.append(new_state)\n",
        "    return tuple(discrete_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be3f7e7",
      "metadata": {
        "id": "7be3f7e7"
      },
      "outputs": [],
      "source": [
        "# Initialize list to store frames for GIF\n",
        "saved_frames = []\n",
        "\n",
        "#### TODO: Implement the Q-Learning algorithm ####\n",
        "# Q-Learning Algorithm for MountainCar with Frame Saving\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()[0]\n",
        "    state = discretize_state(state)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        #### TODO: Implement the epsilon-greedy action selection strategy ####\n",
        "        # Replace the line below with the epsilon-greedy action selection logic.\n",
        "        # Use `random.uniform(0, 1) < epsilon` to decide between exploring and exploiting.\n",
        "        # When exploring, use `env.action_space.sample()` to select a random action.\n",
        "        # When exploiting, select the action with the maximum Q-value using `np.argmax`.\n",
        "        action = env.action_space.sample()  # Temporary default to prevent errors\n",
        "\n",
        "        # Take action, observe new state, reward, done, truncated, and info\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        next_state = discretize_state(next_state)  # Discretize next state\n",
        "\n",
        "        #### TODO: Implement the Q-table update using the Q-learning formula ####\n",
        "        # Replace the line below with the Q-learning formula to update the Q-table.\n",
        "        # Use `alpha`, `gamma`, `reward`, `next_state`, and `np.max()` for the update.\n",
        "        q_table[state][action] = q_table[state][action]  # Temporary placeholder (no-op)\n",
        "\n",
        "        # Update state and reward\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # Check if episode is done or truncated\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    # Append total reward for each episode\n",
        "    episode_rewards.append(total_reward)\n",
        "\n",
        "    # Append current epsilon value\n",
        "    epsilon_values.append(epsilon)\n",
        "\n",
        "    # Epsilon decay\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "    #### TODO: Update the print from 100 to 1000 if your configured episodes exceed 10000 ####\n",
        "    # Print results for every 100 episodes\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode} - Total Reward: {total_reward}\")\n",
        "        frame = env.render()  # Capture frame as image array\n",
        "        saved_frames.append(frame)  # Save frame to list\n",
        "\n",
        "# Close the environment after training\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bea60f11",
      "metadata": {
        "id": "bea60f11"
      },
      "outputs": [],
      "source": [
        "print(\"Training completed for MountainCar!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6449cd3f",
      "metadata": {
        "id": "6449cd3f"
      },
      "source": [
        "Visualization of the results for MountainCar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "# Save frames as a GIF\n",
        "imageio.mimsave('mountain_car_training.gif', saved_frames, fps=15)\n",
        "print(\"GIF saved as 'mountain_car_training.gif'\")"
      ],
      "metadata": {
        "id": "sYP15irirWPC"
      },
      "id": "sYP15irirWPC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# Display the saved GIF\n",
        "Image(filename='mountain_car_training.gif')"
      ],
      "metadata": {
        "id": "TvH3W-f7rvEb"
      },
      "id": "TvH3W-f7rvEb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ded267d4",
      "metadata": {
        "id": "ded267d4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618cdf02",
      "metadata": {
        "id": "618cdf02"
      },
      "outputs": [],
      "source": [
        "# Total Reward per Episode\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(episode_rewards, color='red')\n",
        "plt.title('Total Reward per Episode (MountainCar)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e74478",
      "metadata": {
        "id": "91e74478"
      },
      "outputs": [],
      "source": [
        "# Epsilon Decay over Episodes\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epsilon_values, color='orange')\n",
        "plt.title('Epsilon Decay over Episodes (MountainCar)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Epsilon Value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1fd911",
      "metadata": {
        "id": "2b1fd911"
      },
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Summarizing Epsilon Decay Observations\n",
        "\n",
        "Now that you have implemented epsilon decay and visualized its impact, analyze your results by addressing the following questions:\n",
        "\n",
        "1. **Epsilon Behavior**: How did epsilon change over time? Did it decrease as expected?\n",
        "2. **Exploration vs. Exploitation**: How did the agent's behavior shift as epsilon decreased? Did it explore less and exploit more?\n",
        "3. **Performance Impact**: How did different decay rates affect the agent's learning performance? Was there an optimal decay rate that balanced exploration and exploitation effectively?\n",
        "4. **Real-World Implications**: If you were applying this in a real-world scenario (e.g., robotics, finance, or gaming), how would you justify your chosen epsilon decay strategy?\n",
        "\n",
        "**Action:** Write a short analysis (at least 1-2 paragraphs) summarizing your observations in a markdown cell below.\n"
      ],
      "metadata": {
        "id": "5UiwcT7EmoxA"
      },
      "id": "5UiwcT7EmoxA"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}