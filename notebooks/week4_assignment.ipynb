{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8a748e4-cc9d-47a7-808f-4ac69d8f2b14",
      "metadata": {
        "id": "b8a748e4-cc9d-47a7-808f-4ac69d8f2b14"
      },
      "source": [
        "### Regis University\n",
        "\n",
        "**MSDS688_X70: Artificial Intelligence**  \n",
        "Master of Science in Data Science Program"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0949a7cb",
      "metadata": {
        "id": "0949a7cb"
      },
      "source": [
        "#### Week 4: Natural Language Processing for AI  \n",
        "*GPU Required*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af648df-6808-4636-b3d3-ef7da5120b24",
      "metadata": {
        "id": "8af648df-6808-4636-b3d3-ef7da5120b24"
      },
      "source": [
        "## Lecture: Week 4 - Fine-Tuning GPT-2 and T5 for Text Generation\n",
        "\n",
        "### Overview\n",
        "\n",
        "This week, we focus on two powerful models in **Natural Language Processing (NLP)**: **GPT-2** and **T5**. Both models are widely used for tasks like text generation, summarization, and translation, but they differ significantly in architecture and training approach. We will also discuss how these models can be **fine-tuned** for specific tasks, like generating text from the **Amazon Polarity dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **How GPT-2 Works**\n",
        "\n",
        "#### GPT-2 Architecture:\n",
        "**GPT-2** (Generative Pre-trained Transformer 2) is a **transformer-based** language model that uses a **decoder-only architecture**. GPT-2 is trained in an unsupervised manner on large amounts of text to predict the next word in a sentence, making it excellent for tasks like text generation.\n",
        "\n",
        "#### Key Concepts:\n",
        "- **Unidirectional (Left-to-Right)**: GPT-2 generates text by predicting the next token in a sequence based on all the previous tokens. This makes it ideal for generative tasks.\n",
        "- **Pretraining**: GPT-2 is pretrained on a massive corpus using the task of **next-word prediction** (also called **causal language modeling**). During pretraining, it learns the statistical properties of the language, which can later be fine-tuned for specific tasks.\n",
        "- **Self-Attention Mechanism**: GPT-2 uses self-attention layers to allow the model to focus on different parts of the input sequence when making predictions. This mechanism is key to handling long-range dependencies in text.\n",
        "\n",
        "#### How GPT-2 Generates Text:\n",
        "1. **Tokenization**: The input text is tokenized into smaller units (tokens). These tokens are then fed into the model.\n",
        "2. **Sequential Generation**: Given an input sequence, GPT-2 generates the next token one at a time, based on the context provided by the previous tokens.\n",
        "3. **Sampling Methods**: During text generation, different sampling methods like **top-k sampling** or **nucleus sampling (top-p)** can be used to control randomness and creativity in the generated text.\n",
        "\n",
        "#### Fine-Tuning GPT-2:\n",
        "Fine-tuning GPT-2 involves training it on a new, specific dataset, allowing it to adapt its knowledge to a particular domain or task. This can be done using smaller datasets and fewer resources compared to full pretraining.\n",
        "\n",
        "Steps for Fine-Tuning GPT-2:\n",
        "1. **Load Pretrained Model**: Start with a pretrained GPT-2 model from libraries like Hugging Face.\n",
        "2. **Tokenization**: Preprocess the dataset by tokenizing the text. Since GPT-2 expects input in token format, use its specific tokenizer.\n",
        "3. **Training**: Use the fine-tuned dataset to adjust the weights of the model. The objective is still next-token prediction, but now it’s applied to the new dataset.\n",
        "4. **Evaluation**: Evaluate the fine-tuned model using metrics like **perplexity**, which measures how well the model predicts the next word in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **How T5 Works**\n",
        "\n",
        "#### T5 Architecture:\n",
        "**T5** (Text-To-Text Transfer Transformer) is a transformer model designed to handle **all NLP tasks** in a unified format: **text-to-text**. Unlike GPT-2, T5 uses an **encoder-decoder architecture**, which makes it versatile for tasks like text generation, translation, summarization, and classification.\n",
        "\n",
        "#### Key Concepts:\n",
        "- **Encoder-Decoder**: T5 uses an encoder to process the input sequence and a decoder to generate the output. The encoder captures the context, while the decoder generates the final text.\n",
        "- **Text-to-Text Framework**: In T5, all tasks are cast into a text-to-text format. For instance, a classification task would look like:\n",
        "  ```\n",
        "  Input: \"Is this review positive or negative? Review: The product was great.\"\n",
        "  Output: \"Positive\"\n",
        "  ```\n",
        "- **Pretraining Task (Span Corruption)**: T5 is pretrained using a method called **span corruption**. In this task, random spans of text are replaced with a special token, and the model has to predict the missing text. This makes T5 excellent at understanding context and generating coherent text.\n",
        "\n",
        "#### How T5 Generates Text:\n",
        "1. **Input Encoding**: The input text is tokenized and fed into the encoder, which transforms it into a latent representation.\n",
        "2. **Text Generation**: The decoder then takes the latent representation and generates the output token by token, similar to GPT-2, but using context from the encoder.\n",
        "3. **Task Prefixes**: T5 uses task-specific prefixes (e.g., “summarize:”, “translate English to French:”) to tell the model what type of task it should perform.\n",
        "\n",
        "#### Fine-Tuning T5:\n",
        "Fine-tuning T5 for a specific task is slightly different from GPT-2 because T5 can handle a variety of tasks using the text-to-text framework. You can fine-tune T5 by providing task-specific prompts.\n",
        "\n",
        "Steps for Fine-Tuning T5:\n",
        "1. **Load Pretrained T5 Model**: Start with a pretrained T5 model.\n",
        "2. **Tokenization**: Preprocess the input text and add a task-specific prefix to each input. For example, for text generation tasks, use “summarize:” or “generate:”.\n",
        "3. **Training**: Train the model on the new dataset. The loss function minimizes the difference between the predicted output and the expected output.\n",
        "4. **Evaluation**: T5 can be evaluated using metrics like **perplexity** for text generation or **accuracy** for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Comparing GPT-2 and T5**\n",
        "\n",
        "#### Similarities:\n",
        "- **Transformer Models**: Both GPT-2 and T5 are based on the transformer architecture, which uses attention mechanisms to handle long-range dependencies in text.\n",
        "- **Pretraining and Fine-Tuning**: Both models are pretrained on large corpora and can be fine-tuned for specific tasks with smaller datasets.\n",
        "- **Tokenization**: Both models require tokenizing input text before passing it to the transformer layers.\n",
        "\n",
        "#### Differences:\n",
        "- **Architecture**:\n",
        "  - GPT-2 is a **decoder-only** model (unidirectional), generating text by predicting the next token based on previous tokens.\n",
        "  - T5 uses an **encoder-decoder** architecture (bidirectional), allowing it to understand the context of both input and output sequences.\n",
        "- **Task Flexibility**:\n",
        "  - GPT-2 excels at generative tasks like text completion and creative writing.\n",
        "  - T5 is versatile and can handle both **generation** (e.g., summarization) and **understanding tasks** (e.g., question-answering).\n",
        "- **Pretraining**:\n",
        "  - GPT-2 is pretrained on the task of predicting the next word in a sentence (causal language modeling).\n",
        "  - T5 is pretrained on **span corruption**, making it more robust for understanding context across a broader range of tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Fine-Tuning in This Assignment**\n",
        "\n",
        "In this assignment, you will fine-tune both **GPT-2** and **T5** on a subset of the **Amazon Polarity dataset** for text generation tasks. You will:\n",
        "1. **Tokenize the Dataset**: Prepare the data for both models using their respective tokenizers.\n",
        "2. **Train GPT-2**: Fine-tune GPT-2 using its causal language modeling head to generate text based on the dataset.\n",
        "3. **Train T5**: Fine-tune T5 by framing the task as a text-to-text problem (e.g., \"generate: positive review\").\n",
        "4. **Compare Results**: Evaluate the models on metrics like **perplexity** and compare their performance in terms of inference time and the quality of the generated text.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This week’s assignment dives into the intricacies of **GPT-2** and **T5**, two state-of-the-art transformer models for NLP tasks. You will learn how to fine-tune these models for specific tasks and evaluate their performance. Pay attention to the differences in how they approach text generation, and observe how fine-tuning can adapt these models to new datasets and tasks.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6627c3f9",
      "metadata": {
        "id": "6627c3f9"
      },
      "source": [
        "## Assignment Part 1: Follow Me – Fine-Tuning GPT-2 on a Subset of the Dataset\n",
        "\n",
        "In this section, you will fine-tune the GPT-2 model on a subset of the dataset for text generation tasks. You'll explore how pre-trained language models can be customized for specific tasks by adjusting their weights through fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "ePu73hTpfCqI"
      },
      "id": "ePu73hTpfCqI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c45499",
      "metadata": {
        "id": "c6c45499"
      },
      "outputs": [],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ffd129",
      "metadata": {
        "id": "44ffd129"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "import math\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d29a0a3",
      "metadata": {
        "id": "8d29a0a3"
      },
      "outputs": [],
      "source": [
        "# Load the Amazon Polarity dataset and take a small subset\n",
        "dataset = load_dataset(\"amazon_polarity\")\n",
        "train_dataset = dataset[\"train\"].select(range(1000))  # Select only 1000 examples for training\n",
        "test_dataset = dataset[\"test\"].select(range(200))  # Select only 200 examples for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0e8e17",
      "metadata": {
        "id": "ca0e8e17",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# GPT-2 Tokenizer and Model\n",
        "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)\n",
        "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token  # Set pad token to eos token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca3ceb8",
      "metadata": {
        "id": "aca3ceb8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Preprocess and tokenize for GPT-2\n",
        "def preprocess_gpt2(examples):\n",
        "    inputs = [\"Positive review: \" + example if label == 1 else \"Negative review: \" + example for example, label in zip(examples[\"content\"], examples[\"label\"])]\n",
        "    return tokenizer_gpt2(inputs, padding=\"max_length\", truncation=True, max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c5f0aa",
      "metadata": {
        "id": "d9c5f0aa",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "tokenized_gpt2_train = train_dataset.map(preprocess_gpt2, batched=True)\n",
        "tokenized_gpt2_test = test_dataset.map(preprocess_gpt2, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9954f110",
      "metadata": {
        "id": "9954f110",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# Ensure labels are the same as input_ids for GPT-2 fine-tuning\n",
        "def group_texts(examples):\n",
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()  # Set labels as input_ids for GPT-2 fine-tuning\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbb9287",
      "metadata": {
        "id": "2cbb9287"
      },
      "outputs": [],
      "source": [
        "# Apply labels to both train and test datasets\n",
        "tokenized_gpt2_train = tokenized_gpt2_train.map(group_texts, batched=True)\n",
        "tokenized_gpt2_test = tokenized_gpt2_test.map(group_texts, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fefd62",
      "metadata": {
        "id": "f8fefd62"
      },
      "outputs": [],
      "source": [
        "# Fine-tune GPT-2 on the subset\n",
        "model_gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c79d49f",
      "metadata": {
        "id": "5c79d49f"
      },
      "outputs": [],
      "source": [
        "# Set device (GPU if available, else CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_gpt2 = model_gpt2.to(device)  # Move model to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3244d6c1",
      "metadata": {
        "id": "3244d6c1"
      },
      "outputs": [],
      "source": [
        "training_args_gpt2 = TrainingArguments(\n",
        "    output_dir=\"./results_gpt2\",\n",
        "    eval_strategy=\"epoch\",  # Use eval_strategy instead of deprecated evaluation_strategy\n",
        "    num_train_epochs=1,  # Reduce the number of epochs to speed up training\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b289d8a",
      "metadata": {
        "id": "9b289d8a"
      },
      "outputs": [],
      "source": [
        "trainer_gpt2 = Trainer(\n",
        "    model=model_gpt2,\n",
        "    args=training_args_gpt2,\n",
        "    train_dataset=tokenized_gpt2_train,\n",
        "    eval_dataset=tokenized_gpt2_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d3354d",
      "metadata": {
        "id": "93d3354d"
      },
      "outputs": [],
      "source": [
        "# Train GPT-2\n",
        "trainer_gpt2.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c5da5df",
      "metadata": {
        "id": "9c5da5df"
      },
      "outputs": [],
      "source": [
        "# Generate sentiment-based text using GPT-2 with attention mask and pad_token_id\n",
        "input_ids_positive_gpt2 = tokenizer_gpt2.encode(\"Positive review: \", return_tensors=\"pt\").to(device)\n",
        "attention_mask_gpt2 = input_ids_positive_gpt2.ne(tokenizer_gpt2.pad_token_id).long().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd325a64",
      "metadata": {
        "id": "cd325a64"
      },
      "outputs": [],
      "source": [
        "output_gpt2 = model_gpt2.generate(\n",
        "    input_ids_positive_gpt2,\n",
        "    attention_mask=attention_mask_gpt2,  # Add attention mask\n",
        "    max_length=50,\n",
        "    pad_token_id=tokenizer_gpt2.eos_token_id  # Set pad_token_id to eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f89219",
      "metadata": {
        "id": "f4f89219"
      },
      "outputs": [],
      "source": [
        "generated_text_gpt2 = tokenizer_gpt2.decode(output_gpt2[0], skip_special_tokens=True)\n",
        "print(\"Generated Text from GPT-2:\", generated_text_gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample content:\", train_dataset[0][\"content\"])\n",
        "print(\"Sample label (summary):\", train_dataset[0][\"label\"])"
      ],
      "metadata": {
        "id": "vuJve-zciQfE"
      },
      "id": "vuJve-zciQfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e52222",
      "metadata": {
        "id": "b6e52222"
      },
      "outputs": [],
      "source": [
        "# Evaluate GPT-2\n",
        "eval_results_gpt2 = trainer_gpt2.evaluate()\n",
        "perplexity_gpt2 = math.exp(eval_results_gpt2['eval_loss'])\n",
        "print(f\"Perplexity from GPT-2: {perplexity_gpt2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c92e3e4",
      "metadata": {
        "id": "0c92e3e4"
      },
      "source": [
        "## Assignment Part 2: Your Turn – Fine-Tuning T5 for Text Generation (on a subset)\n",
        "\n",
        "In this section, you will fine-tune the T5 model on a subset of the dataset for text generation. You will compare the performance and text generation capabilities of T5 with the GPT-2 model from Part 1. **A framework has been provided, and your job is to complete the TODOs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf34bbee",
      "metadata": {
        "id": "cf34bbee"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import math\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Amazon Polarity dataset and take a small subset\n",
        "dataset = load_dataset(\"amazon_polarity\")\n",
        "train_dataset = dataset[\"train\"].select(range(1000))  # Select only 1000 examples for training\n",
        "test_dataset = dataset[\"test\"].select(range(200))  # Select only 200 examples for evaluation\n"
      ],
      "metadata": {
        "id": "XZ9d6QaieILQ"
      },
      "id": "XZ9d6QaieILQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pwwNGjj-w3lM",
      "metadata": {
        "id": "pwwNGjj-w3lM"
      },
      "outputs": [],
      "source": [
        "# T5 Tokenizer and Model\n",
        "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nENcMJ2tw5uP",
      "metadata": {
        "id": "nENcMJ2tw5uP"
      },
      "outputs": [],
      "source": [
        "#### TODO: Preprocess and tokenize for T5 model fine-tuning ####\n",
        "def preprocess_t5(examples):\n",
        "    # HINT: Start by preparing input prompts with a phrase that indicates the task, such as \"classify sentiment:\".\n",
        "    # Use the \"content\" field from the dataset for the text.\n",
        "\n",
        "    # HINT: Next, create the target labels as strings. Assign \"positive\" or \"negative\" based on the label value.\n",
        "\n",
        "    # HINT: Tokenize the inputs, setting a maximum length to handle variable text sizes.\n",
        "    # Make sure to use padding and truncation.\n",
        "\n",
        "    # HINT: Tokenize the labels separately from inputs, keeping them shorter since they only contain the target sentiment.\n",
        "    # Include the tokenized labels in `model_inputs` under the key \"labels\".\n",
        "\n",
        "    return model_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_t5_train = train_dataset.map(preprocess_t5, batched=True)\n",
        "tokenized_t5_test = test_dataset.map(preprocess_t5, batched=True)\n"
      ],
      "metadata": {
        "id": "7FhTRZLlu5zP"
      },
      "id": "7FhTRZLlu5zP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune T5 on the subset\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_t5 = model_t5.to(device)\n"
      ],
      "metadata": {
        "id": "-tqlkIrwvRJd"
      },
      "id": "-tqlkIrwvRJd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AWEM1v0Kw6wN",
      "metadata": {
        "id": "AWEM1v0Kw6wN"
      },
      "outputs": [],
      "source": [
        "#### TODO: Set up training arguments for fine-tuning the T5 model ####\n",
        "training_args_t5 = TrainingArguments(\n",
        "    # HINT: Specify the directory path where you want to save the model checkpoints and other outputs.\n",
        "\n",
        "    # HINT: Choose an evaluation strategy to determine how often the model should be evaluated. Consider setting it to evaluate after each epoch.\n",
        "\n",
        "    # HINT: Define the total number of training epochs. Start with a small number if you want faster results or are experimenting.\n",
        "\n",
        "    # HINT: Set the batch size per device (GPU or CPU) to manage memory usage effectively.\n",
        "\n",
        "    # HINT: Specify the number of steps between saving checkpoints to avoid excessive storage use.\n",
        "\n",
        "    # HINT: Limit the total number of checkpoints saved. This prevents storage from being overloaded with old checkpoints.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XBZ-SGEuw8p8",
      "metadata": {
        "id": "XBZ-SGEuw8p8"
      },
      "outputs": [],
      "source": [
        "trainer_t5 = Trainer(\n",
        "    model=model_t5,\n",
        "    args=training_args_t5,\n",
        "    train_dataset=tokenized_t5_train,\n",
        "    eval_dataset=tokenized_t5_test,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train T5\n",
        "trainer_t5.train()"
      ],
      "metadata": {
        "id": "yBcPQQrLtZvM"
      },
      "id": "yBcPQQrLtZvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sentiment-based text using T5\n",
        "sample_review = train_dataset[0][\"content\"]\n",
        "input_text = f\"classify sentiment: {sample_review}\"\n",
        "input_ids_positive_t5 = tokenizer_t5(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "output_t5 = model_t5.generate(input_ids_positive_t5, max_length=10)  # Limiting output length\n",
        "\n",
        "generated_text_t5 = tokenizer_t5.decode(output_t5[0], skip_special_tokens=True)\n",
        "print(\"Generated Text from T5:\", generated_text_t5)\n",
        "\n",
        "print(\"Sample content:\", train_dataset[0][\"content\"])\n",
        "print(\"Sample label (summary):\", train_dataset[0][\"label\"])\n"
      ],
      "metadata": {
        "id": "aalWmuwVuPfz"
      },
      "id": "aalWmuwVuPfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate T5\n",
        "eval_results_t5 = trainer_t5.evaluate()\n",
        "perplexity_t5 = math.exp(eval_results_t5['eval_loss'])\n",
        "print(f\"Perplexity from T5: {perplexity_t5}\")"
      ],
      "metadata": {
        "id": "X8xGkU-Xtdwd"
      },
      "id": "X8xGkU-Xtdwd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec89000c",
      "metadata": {
        "id": "ec89000c"
      },
      "outputs": [],
      "source": [
        "# Imports for Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3251e76",
      "metadata": {
        "id": "d3251e76"
      },
      "outputs": [],
      "source": [
        "# Measure inference time for GPT-2\n",
        "start_gpt2 = time.time()\n",
        "input_ids_positive_gpt2 = tokenizer_gpt2.encode(\"Positive review: \", return_tensors=\"pt\").to(device)\n",
        "attention_mask_gpt2 = input_ids_positive_gpt2.ne(tokenizer_gpt2.pad_token_id).long().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478ed92d",
      "metadata": {
        "id": "478ed92d"
      },
      "outputs": [],
      "source": [
        "output_gpt2 = model_gpt2.generate(\n",
        "    input_ids_positive_gpt2,\n",
        "    attention_mask=attention_mask_gpt2,  # Add attention mask\n",
        "    max_length=50,\n",
        "    pad_token_id=tokenizer_gpt2.eos_token_id  # Set pad_token_id to eos_token_id\n",
        ")\n",
        "time_gpt2 = time.time() - start_gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f61c931",
      "metadata": {
        "id": "5f61c931"
      },
      "outputs": [],
      "source": [
        "generated_text_gpt2 = tokenizer_gpt2.decode(output_gpt2[0], skip_special_tokens=True)\n",
        "print(\"Generated Text from GPT-2:\", generated_text_gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd1fa2e",
      "metadata": {
        "id": "cfd1fa2e"
      },
      "outputs": [],
      "source": [
        "# Evaluate GPT-2 perplexity\n",
        "eval_results_gpt2 = trainer_gpt2.evaluate()\n",
        "perplexity_gpt2 = math.exp(eval_results_gpt2['eval_loss'])\n",
        "print(f\"Perplexity from GPT-2: {perplexity_gpt2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fe1ae0",
      "metadata": {
        "id": "e8fe1ae0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "# Start the timer\n",
        "start_t5 = time.time()\n",
        "\n",
        "# Evaluate T5 perplexity\n",
        "eval_results_t5 = trainer_t5.evaluate()\n",
        "\n",
        "# Stop the timer and calculate elapsed time\n",
        "time_t5 = time.time() - start_t5\n",
        "\n",
        "# Calculate and print T5 perplexity\n",
        "if 'eval_loss' in eval_results_t5:\n",
        "    perplexity_t5 = math.exp(eval_results_t5['eval_loss'])\n",
        "    print(f\"Perplexity from T5: {perplexity_t5}\")\n",
        "else:\n",
        "    print(\"Evaluation loss not found in eval_results for T5.\")\n",
        "\n",
        "# Print evaluation time\n",
        "print(f\"Evaluation Time for T5: {time_t5:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecba3fbe",
      "metadata": {
        "id": "ecba3fbe"
      },
      "outputs": [],
      "source": [
        "# Compare perplexity for GPT-2 and T5\n",
        "models = ['GPT-2', 'T5']\n",
        "training_times = [time_gpt2, time_t5]\n",
        "perplexities = [perplexity_gpt2, perplexity_t5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216e4fb4",
      "metadata": {
        "id": "216e4fb4"
      },
      "outputs": [],
      "source": [
        "# Plot inference time\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(models, training_times, color=['red', 'green'])\n",
        "plt.title('Inference Time (Seconds)')\n",
        "plt.ylabel('Time in Seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6748fc",
      "metadata": {
        "id": "eb6748fc"
      },
      "outputs": [],
      "source": [
        "# Plot perplexity\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(models, perplexities, color=['green', 'orange'])\n",
        "plt.title('Perplexity Comparison')\n",
        "plt.ylabel('Perplexity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "873e7b15",
      "metadata": {
        "id": "873e7b15"
      },
      "outputs": [],
      "source": [
        "# Evaluate Generated Text Quality (GPT-2 and T5 Placeholder)\n",
        "print(\"GPT-2 Generated Text:\")\n",
        "print(generated_text_gpt2)\n",
        "print(\"\\nT5 Generated Text:\")\n",
        "print(generated_text_t5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: T5 Fine-Tuning Analysis and Comparison to GPT-2\n",
        "\n",
        "Now that you've fine-tuned the T5 model on the Amazon Polarity dataset, summarize your observations and learning by addressing the following questions:\n",
        "\n",
        "- **T5 Performance:**  \n",
        "  How well did T5 perform on text generation tasks in terms of perplexity, coherence, and inference time?\n",
        "\n",
        "- **Challenges with T5:**  \n",
        "  What specific challenges or issues did you encounter while fine-tuning T5? How did these differ from your experience observing GPT-2?\n",
        "\n",
        "- **Comparative Insights:**  \n",
        "  Based on your results, how does T5’s encoder-decoder architecture impact its text generation capability compared to GPT-2’s decoder-only design?\n",
        "\n",
        "- **Practical Considerations:**  \n",
        "  Under what circumstances would T5 be a better choice than GPT-2 for real-world NLP applications?\n",
        "\n",
        "**Action:**  \n",
        "Write a brief analysis (1-2 paragraphs) summarizing your findings and insights clearly in a markdown cell below.\n"
      ],
      "metadata": {
        "id": "I_zrD8maxxWa"
      },
      "id": "I_zrD8maxxWa"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}